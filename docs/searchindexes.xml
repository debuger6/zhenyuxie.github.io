<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>Main-Memory Hash Joins on Multi-Core CPUs/ Tuning to the Underlying Hardware 论文解读</title><url>/post/2023-03-23-radix-hash-join/</url><categories><category>数据库</category><category>并行计算</category></categories><tags><tag>hash join</tag><tag>database</tag></tags><content type="html"> 这篇论文介绍了如何在多核 CPU 上结合底层硬件参数（能力）调优 Main-Memory Hash Joins 算法。
介绍 hash join 是一种常见的数据库操作，用于将两个表中的行通过一个共同的 join 键连接起来。hash join 通常可以分为基于磁盘和基于内存两种方式。在基于磁盘的 hash join 中，数据需要从磁盘读入内存，这样可能会导致高延迟和低效率。相比之下，基于内存的 hash join 通常比较快速和高效。
然而，随着多核 CPU 的出现，基于内存的 hash join 也面临着一些新的挑战。在多核 CPU 上，数据的并发读写可能会导致缓存争用和内存带宽瓶颈等问题，从而影响 hash join 的性能。因此，需要对基于内存的 hash join 算法进行优化，以充分利用多核 CPU 的计算能力和内存带宽。该论文旨在提出一种针对多核 CPU 的主存 hash join 方法，并优化该方法以适应底层硬件的特性。
BACKGROUND: IN-MEMORY HASH JOINS 现有的算法可分为两大阵营，一种是 *Hardware-oblivious（硬件无感）*的 hash join，即算法不考虑硬件相关的参数，相反，他们从现代硬件的特征考虑，认为可以在任何技术相似的硬件上取得良好的性能，代表有 no partitioning join。
另一种是 *Hardware-conscious（硬件有感）*的实现，这种方法旨在通过调整算法参数（如 hash table 大小等）来最大化利用硬件特性，代表有 radix join。
Canonical Hash Join Algorithm 任何现代 hash join 实现的基础都是 canonical hash join，canonical Hash Join 算法的主要思想是将两个表的连接操作转化为一个哈希表的建立和查找操作。主要分为两个阶段：build phase 和 probe phase。算法如上图 Fig. 1 所示，步骤如下：
Scan 较小表 \(R\)，并对其中的 join 列建立哈希表，哈希表的键为 join 列的值，哈希表的值为连接列所在行的指针或行的副本 Scan 较大表 \(S\)，对于每一行，使用哈希函数计算连接列的哈希值，查找哈希表中对应的条目，并与当前行进行匹配 如果匹配成功，则将两个表的对应行合并，并输出到结果集中 如果没有匹配，则继续处理下一行 处理完所有行之后，输出结果集 两个表都需要 scan 一次，假设 hash 表的访问是常数时间开销，那么 canonical hash join 算法的时间复杂度为 \(O(|R|+|S|)\)。
No Partitioning Join 为了利用现在并行硬件的优势，业界提出了一种 canonical hash join 算法的变体 no partitioning join。该算法中的 no partitioning 指的是不对哈希表分区，但是会对数据表分区。这种算法不依赖硬件的任何特定参数，该算法如上图 Fig. 2 所示。
两张表都会被均分成多个部分（portions），这些 portions 会分配给多个线程并行处理。在 build phase，所有线程都会 scan 自己负责的 portions 生成一个全局的哈希表。由于哈希表是所有线程共享的，也就意味着向哈希表插入数据需要 synchronized，线程在向哈希表中的 bucket 插入数据时，需要先上锁，上锁成功后才能插入数据。论文指出这里潜在的锁竞争会比较少，因为哈希 bucket 的个数往往比较多，由于锁是 bucket 粒度的，所以锁冲突的概率比较低。
probe phase 访问哈希表是只读模式，因此在该阶段不用加锁，并行访问的效率会很高。假设系统有 \(p\) 个核，那么该算法在理想情况下（build phase 锁冲突概率极低）的时间复杂度为 \(O(1/p(|R|+|S|))\)。
Radix Join 哈希表可以通过 key 的哈希值直接访问，效率很高，但是随机访问内存也会带来 cache misses 问题。论文中引用了其他论文说明了 cache misses 会影响查询性能。业界也证明了当哈希表的大小大于了 cache 大小，几乎每次哈希表的访问都会导致 cache miss。因此，将哈希表划分成 cache 大小的块可以减少 cache misses，从而提高性能。同时，论文还提到在分区阶段，需要考虑 TLB misses 的问题，这样便引出了 radix join 算法。
Partitioned Hash Join 如上图 Fig. 3. 第一阶段，关系表 \(R\) 和 \(S\) 经过哈希函数 \(h1\) 哈希分区为 \(ri\) 和 \(sj\)；在 build phase，针对每个分区 \(ri\) 通过哈希函数 \(h2\) 构建单独的哈希表，由于每个分区对应一个哈希表，这样哈希表的大小便能适配 CPU cache；在 probe phase，scan \(sj\) 并在对应的哈希表中查找匹配的 tuples 做 join。
由于在 partitioning phase，\(R\) 和 \(S\) 使用相同的哈希函数 \(h1\) 做哈希分区，因此可以得出以下结论： $$ ri \Join sj = \emptyset (i\neq j) $$ 通过哈希分区构建哈希表的方式，解决了哈希表过大导致 cache misses 的问题，但同时又引来了新的问题。由于 \(h1\) 的散出可能很大，哈希分区会产生特别多的 partitions，这些 partitions 往往分布在不同的内存页里。我们知道，操作系统里面内存管理的方式是将内存划分为页来管理，进程通过虚拟地址来访问内存时，需要访问页表找到对应的物理页进而找到对应的物理地址，从而实现地址转换。
地址转换的开销是比较大的，操作系统需要通过 TLB 这一高速缓存来缓存经常被用到的页表项，从而提升地址转换的速度。由于 partitions 可能会比较多，从而会占用很多内存页，进而导致内存页数远远超过 TLB 所能缓存的页表项，那么在访问 partitions 时大概率会导致 TLB misses，造成性能问题。
Radix Partitioning 通过简单的哈希分区，会导致 TLB misses 问题，为了解决这一问题，引出了一种新的分区方法 radix partitioning，如上图 Fig. 4. radix partitioning 会分成多轮（pass），每一轮选取 join 键的不同 bits 来哈希分区，下一轮的输出作为下一轮的输入，通过多轮的方式保证最终的散出不超过 TLB 最大缓存页表项数。由于每一轮都是选取的不同位（a different set of bits），类似基数排序每轮选取不同位排序，因此这种分区算法叫做 radix partitioning。
介绍完 radix partitioning，再来讲 radix join 就比较好理解了，步骤和上述类似：
通过 radix partioning，将 \(R\) 和 \(S\) 划分为数量可控的 \(ri\) 和 \(sj\) 对每个 \(ri\) 构建单独的哈希表 scan \(sj\) 在对应的哈希表中查找匹配的 tuples 做 join 在 radix partitioning 的多轮分区中，每轮最大散出时根据硬件参数来固定设置的，那么 paritioning 的轮数为 \(log|R|\)，因此 radix join 的时间复杂度为 \(O((|R|+|S|)log|R|)\)。
radix join 需根据硬件调整的参数如下：
每轮 radix partitioning 的最大散出需要根据硬件支持的 TLB 数来限制 生成的分区大小需要大致（不能大于）和 CPU cache 保持一致 Parallel Radix Join radix join 能够并行化处理，需要将关系表切分成多个子表，将这些子表分配给多个线程，每个线程将自己负责的子表哈希到相应的 partitions，根据上一节讲的，需要考虑硬件参数来限制数量，因此 partitions 的数量不会太多。由于多个线程都会往 partitions 写数据，因此这里存在潜在的竞争问题，由于 partitions 的数量有限，这里竞争的概率就比较大（对比 no partitioning join 低概率竞争）。
为了避免线程竞争问题，这里有个比较有意思的做法是，每个 partiton 预先为每个线程保留相应大小区间范围，然后每个线程计算写入 partiton 的位置，将对应的数据写入相应位置即可。这里由于每个线程写入的位置一定不同，所以可以不用加锁同步。论文其实没有详细讲并行 partitioning 的具体方法，为了便于理解，下面引用 CMU Andy Pavlo 教授的课件来讲解下并行 radix partitioning 的步骤。
第一步，每个线程 scan 子表，然后统计 radix hash 后各个 partiton 的 histogram（其实就是每个 partition 的 tuple 个数），如下图：
第二步，计算每个线程输出的偏移位置，第一步完了后，其实可以得到一个序列如下：
2 1 2 3 // 分别表示 thread-0 在 partition-0, thread-1 在 partiton-0, thread-0 在 partiton-1，thread-1在 partition-1 中的数量 然后根据上述序列可以生成一个序列，叫做 prefixSum 如下：
0 0+2 0+2+1 0+2+1+2 =&amp;gt; 0 2 3 5 // 每个值代表上述序列每个元素位置的前缀和 计算出的 prefixSum 序列就是各线程在 partitons 中的偏移位置，如下：
第三步，每个线程再一次 scan 子表，将数据写入 partitions 相应位置，如下：
最后，选取其他 bit，递归上述的 parallel radix partitioning 步骤，直到得到目标个数的 partitions：
通过上面的讲解，并行 radix partitioning 的实现原理应该比较清晰了。partitioning 完毕后，后面的 build 和 probe 不存在线程竞争，可以抽象成 task，交给线程池并行处理。到此，整篇论文的原理讲解完毕。
小结 本文主要解读了论文的核心原理，论文实验数据部分没有贴上来，大家感兴趣可以查看原始论文。本文整体讲解脉络和论文保持一致，有些地方是笔者根据自己的理解来叙述讲解的，如果不当之处，欢迎指正！
Reference https://15721.courses.cs.cmu.edu/spring2016/papers/balkesen-icde2013.pdf https://15721.courses.cs.cmu.edu/spring2023/slides/11-hashjoins.pdf</content></entry><entry><title>深入理解 Elasticsearch 分页技术</title><url>/post/2023-02-19-es-paginate/</url><categories><category>搜索引擎</category></categories><tags><tag>Elasticsearch</tag><tag>分页</tag></tags><content type="html"><![CDATA[ Elasticsearch 是一款分布式的搜索引擎，提供了灵活的分页技术。本文主要介绍 Elasticsearch（简称 ES） 的几种分页技术，并深入分析各种分页技术的优缺点及应用场景。
介绍 搜索引擎分页是指在搜索引擎中，将搜索结果分成多个页面展示，用户可以通过点击分页按钮或滚动页面来浏览不同页数的搜索结果。搜索引擎通常会在页面底部显示数字标记或其他标记以提示用户当前的位置和可用的页数。如下所示：
搜索引擎分页的目的是提高用户体验，让用户可以更轻松地从大量搜索结果中获取更精准的信息。当用户在搜索框中输入关键字并点击搜索按钮时，搜索引擎将返回与关键字相关的成千上万个搜索结果。如果将所有这些结果显示在一个页面上，页面加载时间会变得非常长，用户体验也会变得糟糕。因此，搜索引擎将搜索结果分页，相关度越高的结果越靠前，这样用户可以更快的找到想要的结果，同时减少页面加载时间和流量消耗。
本文主要关注分页技术，对于搜索引擎其他相关技术不在本文讨论范围，下面将介绍 ES 中的分页技术。
ES 分页 在讲分页技术前，这里简单介绍下 ES 的查询流程，如图：
这里画的是最常见 Query_Then_Fetch 查询流程，整个步骤为：
协调节点接收查询请求 协调节点转发 QueryRequest 请求给数据节点 数据节点执行 QueryPhrase，查询满足条件的 TopN 文档信息（包括id、score，不包括文档内容）返回给协调节点 协调节点接收到数据节点返回的 QueryResults，然后从 k*TopN 的文档信息中选出最终的 TopN 的结果 协调节点发送 FetchRequest 给相关数据节点 数据节点执行 FetchPhrase，根据文档id获取文档内容，然后返回给协调节点 协调节点将最终的查询结果返回给客户端 到这里，我们大体了解了 ES 的查询流程，下面来看下 ES 的分页技术。Elasticsearch 提供了三种分页技术：
From+Size Search After Scroll 下面详细介绍这三种分页技术并深入分析它们的实现原理。
From+Size 介绍 在 ES 中，from 和 size 是两个控制分页查询的参数。from 参数用于指定从第几个文档开始返回结果，size 参数用于指定返回的结果集的大小。
具体来说，from 参数定义了结果集的起始点，而 size 参数定义了结果集的大小。如果想返回结果集中的前 10 个文档，可以将 from 参数设置为 0，size 参数设置为 10。
以下是一个使用 from 和 size 进行分页查询的示例：
GET /my_index/_search { &#34;from&#34;: 100, &#34;size&#34;: 10, &#34;query&#34;: { &#34;match&#34;: { &#34;content&#34;: &#34;chatgpt&#34; } } } 在这个示例中，我们将 from 参数设置为 100，size 参数设置为 10，以便返回结果集中 [100,110) 共 10 个匹配 content 字段的文档，相当于返回第 11 页的结果集。那是不是我们可以在任何分页场景下都可以使用这种分页方式呢，答案是否定的，ES 对 from+size 是有限制的，默认值为 10000。
那问题来了，为什么会有这样的限制呢？这个问题也是本文的重点之一。ES 官方文档给出的解释是：对于深度分页，ES 的每个 shard 都会查询 TopN（查询流程的步骤3，注意 N=from+size）的结果，即查询 [from, from+size) 的结果实际上数据节点会查询 from+size 个结果，也就是将 [0, from) 的结果也一并查出来了，这样将会导致 CPU 和 内存的开销较大，导致系统卡顿甚至 OOM（特别是协调节点，要收集多个 shard 返回的结果，内存开销更大）。因此，from+size 常常应用于分页深度较小的场景，不适合深分页场景。
好了，那问题又来了，明明只查询 size 个结果，为什么每个 shard 偏要将 [0, from+size) 的结果都查出来呢，直接返回 [from, from+size] 的结果不就完了吗？要回答这个问题，首先要来看个简单的例子，如下：
如上图，假设有 3 个shard，每个 shard 的文档按照 value 字段大笑逆序排序，查询的 from 设置为 2，size 设置为 3。那么按照 ES 的处理逻辑，每个 shard 都会返回 [0, 5) 的文档（注意不包含文档内容），协调节点将收到 15 条文档，然后对这 15 条文档按照 value 排序，取前 [2, 5) 的文档为结果。如上图的 result 即为正确结果。
那么如果每个 shard 只返回各自的 [2, 5) 的文档，结果将会如何呢？请看下图：
上图表示每个 shard 返回 [2, 5) 的文档集合并后的结果。很明显，这个结果是不正确的，原因便是每个 shard 并不知道全局的排序结果，因此为了保证能够得到正确的结果，必须返回 [0, 5] 的结果集，因为它们中的任意一个都可能在全局序的 [2, 5） 范围内。
实现原理 上一节介绍了 from+size 的用法和基本原理，这一节来分析下 from+size 在 ES 中是如何实现的，只有真正明白了实现原理，才能更好地掌握使用。下面分几种情况讨论：
不打分场景（打分的特殊场景，打分为常量） 打分场景 排序场景 第一种情况不打分场景， DSL 如下：
{ &#34;from&#34;: 0, &#34;size&#34;: 5, &#34;query&#34;: { // 这里如果没有 query 其实也相当于不打分，因为每个 doc 的打分都是常量 1 &#34;bool&#34;: { &#34;filter&#34;: [ // filter 查询不打分 { &#34;term&#34;: { &#34;clientip.keyword&#34;: &#34;110.47.202.158&#34; } } ] } } } 我们来看下上述 DSL 执行的关键流程，为了便于说明，引用部分源码：
// ContextIndexSearcher.java 该类继承自 lucene 的 IndexSearcher protected void search(List&lt;LeafReaderContext&gt; leaves, Weight weight, Collector collector) throws IOException { for (LeafReaderContext ctx : leaves) { // search each subreader searchLeaf(ctx, weight, collector); } } leaves 代表 lucene 的 segment，在该方法中会遍历查询每个 segment。本文的重点不是源码解析，这里只贴少量代码，我们直接看查询 segment 的关键代码：
// DefaultBulkScorer.scoreRange 方法部分代码 while (currentDoc &lt; end) { if (acceptDocs == null || acceptDocs.get(currentDoc)) { collector.collect(currentDoc); } currentDoc = iterator.nextDoc(); } 这里开始遍历 segment 中的 doc，然后用 collector(shard级，所有segment共用) 取收割 doc，这里的 collector 实际是 SimpleTopScoreDocCollector.getLeafCollector 获取的匿名类对象。collector.collect 的逻辑如下：
public void collect(int doc) throws IOException { float score = scorer.score(); totalHits++; hitsThresholdChecker.incrementHitCount(); if (minScoreAcc != null &amp;&amp; (totalHits &amp; minScoreAcc.modInterval) == 0) { updateGlobalMinCompetitiveScore(scorer); } // pqTop 的doc和score初始为&lt;2147483647, 负无穷&gt;，只有当pq满后，才会将其挤出去 if (score &lt;= pqTop.score) { if (totalHitsRelation == TotalHits.Relation.EQUAL_TO) { updateMinCompetitiveScore(scorer); } return; } // 把 doc 加入 pqTop pqTop.doc = doc + docBase; pqTop.score = score; pqTop = pq.updateTop(); // 关键，更新文档所需最小的 score updateMinCompetitiveScore(scorer); } 所有 collector 都会维护一个堆 pq 来收集 topN 的 doc
这里关注下方法 updateMinCompetitiveScore，当 pq 满，下一个 doc 进来会走到第 14 行，里面会更新所需 doc 的最小 score 值为大于 pqTop.score 的第一个 float 值，而且由于是不打分场景，所以还会调用 DocIdSetIterator.empty 重新生成一个 iterator，这个 iterator 的 nextDoc 方法永远只返回一个常量 NO_MORE_DOCS(2147483647)，因此外层 while 继续遍历时将终止循序。如果当前 segment 遍历完毕后，pq 还没满，那么将继续查询下一个 segment。
好了，如果查询完当前 segment，pq 已经满了，那剩下的 segment 还需要继续遍历吗？答案是否定的，原因是，查询下一个 segment 时，调用 collector.setScorer 内部会调用 updateMinCompetitiveScore 生成一个永远只返回 NO_MORE_DOCS 的 DocIdSetIterator，因此，scoreRange 里面的 while 循环永远走不进去。
通过上述的分析，from+size 在这种不打分的场景下，性能是最好的。
第二种情况打分场景，DSL 如下：
GET kibana_sample_data_logs_1/_search { &#34;from&#34;: 0, &#34;size&#34;: 5, &#34;query&#34;: { &#34;match&#34;: { &#34;message&#34;: &#34;GET&#34; } } } 这种场景也是使用 SimpleTopScoreDocCollector 来收割 doc，所以也是走的第一种情况的 collect 方法，不同的是，当 pq 满后，还会继续遍历 doc，因为后面可能还有 doc 的 score 大于 pqTop.score。因此，打分场景需要遍历完所有 segment 的所有 doc 才能确定最终的结果。不难看出，该场景的性能会比上一种场景性能差，所以，如果没有打分需求，应该避免使用打分的 query 语句。
第三种场景排序场景，DSL 如下：
GET kibana_sample_data_logs_1/_search { &#34;from&#34;: 0, &#34;size&#34;: 5, // query（打分或不打分） 省略 &#34;sort&#34;: [ { &#34;bytes&#34;: { &#34;order&#34;: &#34;desc&#34; } } ] } 查询时指定了排序字段，那么便需要按排序字段的顺序返回 topN 的结果。这种场景会使用 SimpleFieldCollector 来收割 doc，代码太长，这里不贴代码了，collect 的逻辑和上面场景类似，不同的是比较当前 doc 和 pqTop 时，比较的是排序字段的 docValue。
如果文档写入时不是按查询时的排序字段排序的，那么将会遍历完 shard 的所有 doc 才能得到最终结果。反之，如果文档本来是按排序字段排序的，那么查询时 collect 里面有个优化分支，当 pq 满后，会触发提前终止，裁剪掉当前 segment 剩下的 doc，查询后面的 segment 时，逻辑一样。
Search After 介绍 上一小节介绍了 from+size 的原理，也分析了它的弊端 - 即不适合深分页。好了，如果我们需要获取前 1000 页，每页 10 条文档怎么办？针对这种深分页场景，ES 提供了一种新的分页方式 – search_after。Elasticsearch 中的 search_after 机制是一种更有效的分页方法，它可以在不加载整个数据集的情况下快速地获取下一页数据。
search_after 是一种基于游标的分页方法，使用 search_after 查询时必须指定排序字段（可以有多个），它使用排序字段值作为游标，从而能够更快地获取下一页的数据。在进行第一次搜索时，ES 会返回第一页的结果，当需要获取下一页数据时，可以使用上一页最后一个文档的排序字段值作为游标进行搜索。通过这种方式，可以逐步遍历整个数据集而无需一次性加载所有数据。
使用 search_after 机制需要注意以下几点：
搜索请求必须指定排序字段，用于指定搜索结果的顺序 搜索第一页不必指定 search_after 参数，从第二页开始必须指定 search_after 为上一页的最后一个游标 游标必须是唯一的，否则可能会出现重复的数据 search_after 用法举例：
GET twitter/_search { &#34;query&#34;: { &#34;match&#34;: { &#34;title&#34;: &#34;elasticsearch&#34; } }, &#34;sort&#34;: [ {&#34;date&#34;: &#34;asc&#34;}, {&#34;tie_breaker_id&#34;: &#34;asc&#34;} ] } tie_breaker_id 是 _id 的拷贝，开启 doc_values 用于排序
tie_breaker_id 的目的是保证游标的唯一性，继续搜索下一页：
GET twitter/_search { &#34;query&#34;: { &#34;match&#34;: { &#34;title&#34;: &#34;elasticsearch&#34; } }, &#34;search_after&#34;: [1463538857, &#34;654323&#34;], // 上一页最后一个排序字段的值 &#34;sort&#34;: [ {&#34;date&#34;: &#34;asc&#34;}, {&#34;tie_breaker_id&#34;: &#34;asc&#34;} ] } 可以看到，search_after 的使用特别灵活，只要指定了游标值，便能根据游标值查询下一页文档。由于查询过程中，可能还会有数据写入，那么多次查询使用一个游标可能得到的结果不一致，如果业务有一致性需求，需要使用 point in time(PIT) 来创建一个临时的快照，查询时使用该快照保证数据一致性。
实现原理 search_after 使用 PagingFieldCollector 来收割 doc，原理和 from+size 的第三种情况类似，收割从 doc 0 开始，不同的是，collect 时会多一次过滤，即会比较当前 doc 的排序字段值和 search_after 值的大小，如果不满足条件则直接过滤掉，也就意味着 pq 操作更少。因此，整体来看性能会更好。
当然，如果写入的数据已按排序字段排序，那么当 pq 满后，会走优化分支触发提前终止，裁剪掉当前 segment 剩下的 doc。
Scroll 介绍 上一节讲了 search_after 可以用来做深分页，ES 还提供了一种分页技术 – Scroll 查询，Scroll 查询是一种在 ES 中扫描大量数据的常用方法。它通过在搜索结果中建立一个保持状态的 scroll_id 来实现。当您开始滚动时，ES 会返回第一批结果，并返回一个保持状态的 ID。使用此 ID，可以执行下一个滚动请求，以检索下一批结果。此过程可以重复进行，直到所有数据都被扫描完毕为止。
Scoll 查询的使用例子如下：
POST /my-index-000001/_search?scroll=1m { &#34;size&#34;: 100, &#34;query&#34;: { &#34;match&#34;: { &#34;message&#34;: &#34;foo&#34; } } } 第一次查询要指定 scroll 参数，参数值代表 scroll 上下文的保留时长，保留时间过期后，scroll_id 将失效。接着继续查询下一批数据：
POST /_search/scroll { &#34;scroll&#34; : &#34;1m&#34;, &#34;scroll_id&#34; : &#34;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==&#34; } scroll 的用法很简单，除了第一次查询，后面的查询每次传入 scroll_id 便可以拉取下一页数据。由此可以看出，scroll 只能顺序往下翻页，不能往回翻。
实现原理 分三种情况讨论：
不带 query，不带 sort 带 query，不带 sort 带 sort Scroll 查询会保留一个上下文 ScrollContext，上下文中包含了快照信息和上一页文档信息。
第一种情况其实就是将全量数据按存储顺序往后翻页，这种情况使用 PagingTopScoreDocCollector 来收割 doc，特殊的是收割的 doc 不再时从 0 开始，而是从上一页最后一个 doc+1 开始，往后收割 size 个 doc 即完成本次 scoll。由此可以看出，对于这种情况，查询效率非常高。
第二种情况会使用 PagingTopScoreDocCollector 来收割 doc，收割从 doc 0 开始，由于 Scroll 上下文保存了上一页 doc 信息（pq中存储了上一页的doc），会过滤掉分数低于 pqTop 的 doc（如果当前 doc 分数和 pqTop 分数相同，则比较 doc 值）。
第三种情况会使用 PagingFieldCollector 来收割 doc，收割从 doc 0 开始，收割的逻辑和 search_after 类似，collect 时会比较上一页最后一个 doc 的排序字段值，如果不满足条件则过滤掉当前 doc。
小结 这节介绍了三种分页方式的概念和原理，可以得出下面结论：
from+size 适合翻页灵活且页数不大的场景，即适合浅分页场景 search_after 适合深分页，也可以来回翻页（需要业务存储每页所需的search_after） scroll 可以做分页，但是只能顺序往后翻页，不够灵活。所以它最适合做大量数据的导出（对顺序无要求，scan完所有数据即可） 总结 本文主要介绍了 Elasticsearch 中三种分页方式的概念，并详细分析了它们各自的实现原理，最后分析了它们各自适合的使用场景。
Reference https://www.elastic.co/guide/en/elasticsearch/reference/current/paginate-search-results.html#paginate-search-results ]]></content></entry><entry><title>关于我</title><url>/about.html</url><categories/><tags/><content type="html"> “上士闻道，勤而行之；中士闻道，若存若亡；下士闻道，大笑之。不笑不足以为道。” —《道德经》
“知不足而奋进，望远山而前行。” —《人民日报》
“与其焦虑，不如努力；保持热情，保持前行。很少有横空出世的幸运，更多的是不为人知的努力。行动，才不会让梦想止步于空想。当你觉得累的时候，也许，你正走在上坡路。” —《人民日报》
debuger, 热爱技术，热爱学习，乐于分享，主要专注于数据库内核、搜索引擎领域。愿和大家一起交流学习，也请诸君不怜赐教。</content></entry><entry><title>Merge Path - A Visually Intuitive Approach to Parallel Merging 论文解读</title><url>/post/2023-02-15-merge_path/</url><categories><category>数据库</category><category>排序算法</category></categories><tags><tag>mergesort</tag><tag>database</tag></tags><content type="html"><![CDATA[ 在看DuckDB技术博客时，了解到这篇论文。文章提出的算法非常实用，在工程实践中应用效果也很好。本文主要对论文核心部分进行解读，并会根据自己的理解加以叙述，若有不当之处，欢迎纠正。
介绍 在工程实践中，合并两个有序数组是非常常见的操作。最直观的实现方式就是采用一次归并排序，直接将两个有序数组合并生成一个新的有序数组。这种方式简单直观，也最容易实现，在数据集比较小的时候，性能特别好，时延几乎不计。但是在数据集较大的情况下，对于数据库这种对时延较为苛刻的系统来说，时延则不得不成为一个关注点。
在现代CPU多核架构下，可以充分利用CPU并行计算能力，采用多线程并行合并可以成倍提升性能，降低时延。
给一个长度为 \(n\) 的乱序数组，将其排序，考虑merge-sort算法，需要 \(\log_2n\) 轮排序，第一轮排序需要将 \(n/2\) 个 \(pairs\) 比较排序，第二轮需要将 \(n/4\) 个 \(pairs\) 排序，最后一轮将 \(2\) 个 \(pairs\) 排序生成单独的排序数组。
考虑使用并行merge-sort算法，使用 \(p\) 核计算机，假设 \(n \gg p\)，在前面的轮次，每个核可以负责一批 \(pairs\) 的排序工作，越往后需要排序的 \(pairs\) 更少，那么并不是每个核都能分得 \(pairs\)，不能充分发挥并行化计算能力。为了有效地发挥并行计算能力，需要对有序数组的合并也并行化，这样每个核在任何时候都能参与计算。
论文认为一个高效的并行合并算法必须具备如下特征：
所有核拥有相同的工作量（负载均衡） 尽可能少的核间通信 最小化额外工作（为了并行化所做的额外工作） 高效内存访问（高缓存命中率和最小的缓存一致性开销） 一种简单的方法是将两个有序数组按照核数平均切割成多个子数组，然后每个核分配一对长度相同的子数组，各个核排序生成一个有序数组，再将所有有序数组串联起来便得到最终的排序数组。如下：
arr1 = [1,3,5,7,9,11,13,15,17] split to --&gt; [1,3,5] [7,9,11] [13,15,17] arr2 = [2,4,6,8,10,12,14,16,18] split to --&gt; [2,4,6] [8,10,12] [14,16,18] core0| core1| core2| [1,2,3,4,5,6, 7,8,9,10,11,12, 13,14,15,16,17,18] 如上，假设有3核，两个数组都等分成3个子数组，然后每个核合并一对子数组，最终得到一个合并完成的有序数组。一切看起来都没有问题。但是假设两个数组如下：
arr1 = [20,21,22,23,24,25,26,27,28] split to --&gt; [20,21,22] [23,24,25] [26,27,28] arr2 = [10,11,12,13,14,15,16,17,18] split to --&gt; [10,11,12] [13,14,15] [16,17,18] core0| core1| core2| [10,11,12,20,21,22, 13,14,15,23,24,25 16,17,18,26,27,28] 很不幸，上面的排序结果不对，因此，正确的数组划分成为了关键点。
论文提出了一种用于并行随机存取机 (Parallel Random Access Machines, PRAM) 的并行合并算法，即允许并发（并行）访问内存的共享内存架构。论文的算法具备负载均衡、无锁、高效内存访问的特点，仅需少量额外工作。为了实现高效的并行算法，论文提出了一种叫做 Merge Path 的方法，这也是论文的核心，下文将详细分析 Merge Path 的原理。
Merge Path 假设两个有序数组 \(A\) 和 \(B\)，长度分别为 \(|A|\) 和 \(|B|\)， \(A\) 和 \(B\) 构成矩阵 \(M\)，如下图所示：
声明：本文所有图片都来源于原论文。
我们来看下 Merge path 是如何构造的。要合并两个有序数组，我们从起点(1, 1)开始，比较 \(A[1]\) 和 \(B[1]\)，如果 \(A[1]&gt;B[1]\)，将位置往右移动，否则往下移动，重复上述步骤直到走到右下方终点。最终得到一条路径（图中黄色路径），这条路径就称为 Merge Path。
通过构造 Merge Path 可以得出下面几个结论：
引理1：从头到尾遍历 Merge Path 的过程中，向右则选取 \(|B|\) 中未使用的最小值，向下则选取 \(|A|\) 中未使用的最小值，最终得到的结果即为合并后的有序数组。
引理2：合并路径上，任何连续片段都由 \(A\) 的连续元素序列和 \(B\) 的连续元素序列组成。
引理3：合并路径的非重叠片段由不相交的元素集组成，反之亦然。
引理4：对于合并路径上两个不相交的片段，后面片段的元素集肯定大于或等于前面片段的元素集。
定理5：考虑一组子数组对（来源于 \(A\) 和 \(B\))，这些子数组对包含了所有元素，它们一旦排序便形成了合并路径的连续片段。可以断言，这些子数组对可以并行合并，合并后的结果拼接在一起便形成了最终的有序数组。
推论6：将合并路径划分为任何个不重叠的片段，这些片段对应的子数组对都可以独立合并，合并后的结果按片段顺序串联形成一个合并路径上有序数组。
推论7：将合并路径平均划分为相同大小的片段，并行地合并片段对应的数组对，可以在多个处理器间达到负载均衡的效果。
并行合并和排序 有了上面的结论，我们来看下如何实现高效的并行合并算法。根据推论6和7，可以知道，需要将合并路径按照cpu核数平均划分，得到大小均匀的片段，然后并行地合并这些片段对应的数组对即可。要均匀地划分合并路径，其实就是要找到这些划分点，因此，算法的核心就是找这些划分点。算法核心思想如下：
根据核数计算出每个线程处理的数据量 对于每个线程，计算其所处理的起点偏移 对于每个线程，根据起点偏移计算其在合并路径上对应的坐标点\((x, y)\) 对于每个线程，根据自身的起点坐标点和终点坐标点(下一个线程的起点)合并数组对 对于每个线程，将合并的结果写入最终数组对应的下标即可（根据前面的引理可知不同线程片段对应的元素集不会重复，因此这里不会出现内存并发安全问题） 通过上面的算法便能找到所有划分点，如下图：
划分点将合并路径平均分割，然后由每个线程独立地合并排序每个片段对应的数组对，得到最终的合并数组。论文提供了算法的实现代码，见 github地址 。笔者测了下，有点小bug（在某些情况下可能会coredump，笔者用Golang重新实现了算法，见 github地址 ）。
测试 测试环境：MacBook Air M1 芯片，逻辑核数 8
测试数据量：\(A\) 和 \(B\) 的size均为1亿，合并成2亿的数组
直接用论文提供的代码测试，测试结果如下：
$ make test100m ./a.out -A 100000000 -B 100000000 -t 2 OpenMP MergePath Implementation Merging: A[100000000] B[100000000] to C[200000000] using 2 threads serial merge 0.745051 Total OpenMP MergePath 0.379064 Speedup over serial merge 1.965501 MERGE SUCCESSFUL Total Time 0.437201 ./a.out -A 100000000 -B 100000000 -t 4 OpenMP MergePath Implementation Merging: A[100000000] B[100000000] to C[200000000] using 4 threads serial merge 0.745274 Total OpenMP MergePath 0.195741 Speedup over serial merge 3.807443 MERGE SUCCESSFUL Total Time 0.231412 ./a.out -A 100000000 -B 100000000 -t 8 OpenMP MergePath Implementation Merging: A[100000000] B[100000000] to C[200000000] using 8 threads serial merge 0.750698 Total OpenMP MergePath 0.132132 Speedup over serial merge 5.681429 MERGE SUCCESSFUL Total Time 0.160297 ./a.out -A 100000000 -B 100000000 -t 16 OpenMP MergePath Implementation Merging: A[100000000] B[100000000] to C[200000000] using 16 threads serial merge 0.750842 Total OpenMP MergePath 0.126917 Speedup over serial merge 5.916009 MERGE SUCCESSFUL Total Time 0.164079 线程数从2到16递增，线程数越多，并发数越大，MergePath 算法也就越快。当然线程数并不是越多越好，最佳线程数和CPU配置有关系。从上面的结果可以看出，当线程数为16时，并行Merge比普通Merge快了将近6倍，说明并行Merge的实践表现效果非常优秀。
小结 本文主要解读了论文核心部分，还有些遗留内容并未解读，如果想了解更详细内容，可以直接看原论文。同时，笔者用Golang实现了论文中的算法（更加易懂），源码已上传github，感兴趣的同学可以阅读参考。
Reference https://arxiv.org/pdf/1406.2628.pdf https://github.com/ogreen/MergePathOMP ]]></content></entry><entry><title>lucene 编码技术 - DirectMonotonicWriter</title><url>/2022/12/13/lucene-dmw/</url><categories><category>lucene</category></categories><tags><tag>lucene</tag></tags><content type="html"><![CDATA[ 从名称可以看出，DirectMonotonicWriter 这个类主要是针对单调递增或者递减（即有序）数据集的编码。本文将结合源码分析其编码原理。
无序数组压缩 在 Lucene 中，有很多地方都需要存储无序整型序列，如数值型 DocValues 的值、ords 等等。对于这种类型的序列值如何才能做到较好的压缩编码呢？最简单的做法就是减去序列中的最小值，缩小值域范围，比如下面一组值：[35, 40, 30, 45]，值域范围[30, 45]，采用 DirectWriter 编码，bitPerValue 为 8 ；减去最小值后为 [5, 10, 0, 15]，值域为[0, 15]，bitPerValue 为 4，因此相比原始数据可以减少一半的存储开销。
那还能不能取得更进一步的压缩率呢？答案是有的。我们观察上面差值后的数据集，发现它们都能被 5 整除，即最大公约数（gcd）为 5，因此可以再将上述数据集中的数除以 5 得到 [1, 2, 0, 3]。很明显，数据集的范围降低了数倍，此时 bitPerValue 为 2，相比原始数据，可以有 4 倍的压缩比。
综上，对于无序数据集的压缩方式可以命名为差值&amp;最大公约数压缩。bitPerValue 的计算方式为 (max -min)/gcd。
有序数组压缩 上一节讲了无序数据集编码原理，那么对于有序数据集是否可以采用同样的编码方式来达到最佳的编码效果呢？假如有如下数据集 [0, 2, 5, 7, 8]，要达到压缩的效果就是要缩小数据集的值域，对于有序数组，很容易想到可以直接求相邻元数的差值得到 [0, 2, 3, 2, 1]，完了以后再将该数据集按照无序数组的压缩方式压缩即可。
假如有数组 [0, 100, 300, 700, 801]，它们的差值为 [0, 100, 200, 400, 101]，最大差值为 400 依旧较大，bitPerValue 得取 12，范围压缩不够明显。那有没有更好的办法将数组的值域变得更小呢？这就是 DirectMonotonicWriter 需要解决的问题，我们结合源码来看看 DirectMonotonicWriter 是怎么做的。
private void flush() throws IOException { assert bufferSize != 0; // 求 avgInc final float avgInc = (float) ((double) (buffer[bufferSize - 1] - buffer[0]) / Math.max(1, bufferSize - 1)); // 按位置减去 i*avgInc for (int i = 0; i &lt; bufferSize; ++i) { final long expected = (long) (avgInc * (long) i); buffer[i] -= expected; } long min = buffer[0]; for (int i = 1; i &lt; bufferSize; ++i) { min = Math.min(buffer[i], min); } long maxDelta = 0; // 减去最小值 for (int i = 0; i &lt; bufferSize; ++i) { buffer[i] -= min; // use | will change nothing when it comes to computing required bits // but has the benefit of working fine with negative values too // (in case of overflow) maxDelta |= buffer[i]; } meta.writeLong(min); meta.writeInt(Float.floatToIntBits(avgInc)); meta.writeLong(data.getFilePointer() - baseDataPointer); if (maxDelta == 0) { meta.writeByte((byte) 0); } else { final int bitsRequired = DirectWriter.unsignedBitsRequired(maxDelta); DirectWriter writer = DirectWriter.getInstance(data, bufferSize, bitsRequired); for (int i = 0; i &lt; bufferSize; ++i) { writer.add(buffer[i]); } writer.finish(); meta.writeByte((byte) bitsRequired); } bufferSize = 0; } 我们主要关注前面几步，第一步算出数组的平均增长值 avgInc，第二步根据元素所处位置减去 iavgInc。还是以上面的数组 [0, 100, 300, 700, 801] 为例，算出 avgInc = (801-0)/(5-1) = 200，各元数减去 iavgInc 后得到差值数组 [0, -100, -100, 100, 1]；第三步差值数组减去最小值（当前最小值 -100）得到数组 [100, 0, 0, 200, 101]，最大差值（maxDelta）为 200，bitsRequired 为 8。
DirectMonotonicWriter 尽可能让处理后的数组最大元数更小，从而减小编码所需位数。
总结 本文结合例子讲解了无序数组和有序数组的编码原理 ，并着重通过源码讲解了 DirectMonotonicWriter 如何对有序数组进行编码。总的来说，编码方式很容易理解，在 lucene 中多处地方会用到本文讲的编码算法，后面涉及相关内容时会更容易理解。
]]></content></entry><entry><title>lucene 编码技术 - DirectWriter</title><url>/2022/12/04/lucene-dw/</url><categories><category>lucene</category></categories><tags><tag>lucene</tag></tags><content type="html">DirectWriter 这个类的主要作用是将 long[] 型数据集编码存储到 byte[] 中，其实现充分考虑压缩比和性能因素。
编码原理 DirectWriter 使用的是固定位编码方式，即数据集中的所有元数均按照相同的固定位编码到 byte 数组。固定位的值 bitPerValue 只支持下面几种：
static final int[] SUPPORTED_BITS_PER_VALUE = new int[] {1, 2, 4, 8, 12, 16, 20, 24, 28, 32, 40, 48, 56, 64}; 至于 bitPerValue 为什么只支持上述 SUPPORTED_BITS_PER_VALUE 中的值，后面会详细分析。
bitPerValue bitPerValue 即每个 value 所需要的 bit 位数，数据集中的所有数值统一按照 bitPerValue 来编码。那么对于一个数据集，如何确定其对应 bitPerValue 的值呢？很明显，bitPerValue 要能够编码数据集中任意值，必须以数据集中最大值来计算位数。举个栗子，有如下 long 数组：
long []values = {6, 2, 110}; 我们用二进制来表示每个 value，如下：
value binary 6 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000110 2 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000010 110 00000000 00000000 00000000 00000000 00000000 00000000 00000000 01101110 可以看出 6 有三位有效位（去掉前导 0），2 有两位有效位，110 有七位有效位。所以对于该数据集，bitPerValue 应该取值为 7。
上面计算出了数据集的 bitPerValue，那么又出现一个问题，因为 bitPerValue 的值 7 不在 SUPPORTED_BITS_PER_VALUE 中，那又该如何处理呢？这里结合源码来看看。
private static int roundBits(int bitsRequired) { int index = Arrays.binarySearch(SUPPORTED_BITS_PER_VALUE, bitsRequired); if (index &amp;lt; 0) { return SUPPORTED_BITS_PER_VALUE[-index - 1]; } else { return bitsRequired; } } DirectWriter 提供了一个方法 roundBits，它可以根据 bitsRequired 的值从 SUPPORTED_BITS_PER_VALUE 中选取合适的值作为最终的 bitsRequired。逻辑很简单，因为 SUPPORTED_BITS_PER_VALUE 是有序数组，可以使用二分查找判断 bitsRequired 是否在数组中，如果在则直接返回 bitsRequired；如果不在，binarySearch 返回的值是 bitsRequired 插入点（第一个大于 bitsRequired 的索引位置）的负值，最终返回的就是第一个大于 bitsRequired 的数组元素。
所以，这里 bitPerValue 为 7 时，会从 SUPPORTED_BITS_PER_VALUE 中选取 8 作为其最终值。那编码后字节数组如下：
|- 2 -|- 6 -|- 110 -| |00000110|00000010|01101110| 数据集编码后占用 3 bytes，而之前占用 24 bytes，压缩了 8 倍。
上面讲完了 DirectWriter 的编码原理，可以看到其编码原理还是很简单的，比较核心的一点就是 SUPPORTED_BITS_PER_VALUE 的选取，这也是本篇本章讨论的重点，下面会详细分析为什么 bitPerValue 只能选取 SUPPORTED_BITS_PER_VALUE 中的值。
SUPPORTED_BITS_PER_VALUE 本节我们举几个栗子来说明 SUPPORTED_BITS_PER_VALUE 的作用。
栗子1：
假设有如下数据集1：
long []values = {117, 110, 99}; 上述数据的二进制表示如下：
value binary 117 00000000 00000000 00000000 00000000 00000000 00000000 00000000 01110101 110 00000000 00000000 00000000 00000000 00000000 00000000 00000000 01101110 99 00000000 00000000 00000000 00000000 00000000 00000000 00000000 01100011 最大值为 117，计算出 bitPerValue 为 7。假设我们不用 SUPPORTED_BITS_PER_VALUE，直接按 7 bits 来编码数据集，则编码后的字节数组如下：
从上图可以看到，除了第一个数只存入到一个字节中，其他两个数都存入到了两个字节。这也就意味着，要操作两个 byte 才能将 110 和 99 写入或读出。如果使用 SUPPORTED_BITS_PER_VALUE，那么将按 8 bits 来编码数据集，编码后的字节数组如下：
可以看到每个字节存一个数，那么在写入和读取时，都分别操作一个 byte 就可以了，相对上面 7 bits 的方式有更高的性能优势。
同理，对于 SUPPORTED_BITS_PER_VALUE 中的 16、24、32、40、48、56、64 这些 8 的倍数值，起到的效果和上述类似；1、2、4 这三个值也容易理解，因为它们都能整除 8 ，所以一个 byte 可以存放整数个位数为 1 或 2 或 4 的值，不会出现跨 byte 的情况 。但是对于 12、20、28 这三个值如何理解呢？为了解释这个问题，我们再举一个栗子：
假设有数据集2：
long []values = {309, 36, 293, 108}; 上述数据的二进制表示如下：
value binary 309 00000000 00000000 00000000 00000000 00000000 00000000 00000001 00110101 36 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00100100 293 00000000 00000000 00000000 00000000 00000000 00000000 00000001 00100101 108 00000000 00000000 00000000 00000000 00000000 00000000 00000000 01101100 最大值 309，bitPerValue 为 9，如果我们按 9 bits 来编码数据集，则编码后的字节数组如下：
由于 9 bits 大于一个字节，每个数值都必须用 2 个 byte 来存储，跨字节这个问题无法避免。我们再观察上面几个数的存储还有其他弊端没？可以看到，除了第一个数的起始偏移是按字节对齐的，其他数的起始偏移都没按字节对齐，这就意味着如果要读取这些数，首先要定位到起始偏移所在的 byte，然后再定位 byte 内的偏移，因此定位起始偏移的开销相对较大。
既然这样，我们直接用 16 bits 来编码是否更好呢？用 16 bits 来存储确实性能会更好，因为计算起始偏移的开销更低了。但是压缩率又变差了，16 bits 编码需要 8 bytes，而 9 bits 编码只需要 5 bytes。 那我们有没有折中的方案呢，既能够取得较好的压缩率，又有很好的读写性能。
我们看看使用 SUPPORTED_BITS_PER_VALUE 后的效果，如果是 SUPPORTED_BITS_PER_VALUE，那么便会用 12 bits 来编码，编码后的 byte 数组如下：
从上图可以看出，使用 12 bits 编码后，每两个数中有一个数计算起始偏移时需要计算 byte 内偏移，而使用 9 bits 编码是每 8 个数中有 7 个需要计算 byte 内偏移。所以相对 9 bits 编码在这块的开销比为 1/2 : 7/8 = 4 : 7，而压缩比为 5 : 6。因此使用 12 bits 编码既能取得接近 9 bits 相近的压缩率，还能有更好的性能。
同理，20、28 bits 编码解决的问题和 12 bits 类似，这里不在赘述。
总结 本文结合源码和例子详细分析了 DirectWriter 的编码原理，特别对 SUPPORTED_BITS_PER_VALUE 的作用进行了深入分析。DirectWriter 在 lucene 中多处被使用，大家如果想了解更细节地实现可以查看相关源码。</content></entry><entry><title>Pebble 源码剖析 - Skiplist</title><url>/2022/02/26/pebble-skiplist/</url><categories><category>pebble</category></categories><tags><tag>Pebble</tag></tags><content type="html"><![CDATA[本章主要从源码角度来分析 Pebble 对 Skiplist（跳表）的高性能设计，对于跳表的介绍及原理可以参考 百科 或网上其他文章，本文不做冗余介绍。
数据结构 容易想到，跳表和普通链表类似，由多个节点组成，不同的是，每个节点有多层，因此会有多个 next 指针指向每层的下个节点，如下图：
跳表的逻辑结构很容易理解，使用链表这种数据结构实现非常直观。但是 Pebble 从高性能的角度出发，底层存储并未使用链表，而使用的是数组，通过 CAS 无锁技术，在并发场景下具备高效的性能。下面先来看看源码中几个关键的数据结构。
Skiplist Skiplist 源码定义如下:
type Skiplist struct { arena *Arena cmp base.Compare head *node tail *node height uint32 // Current height. 1 &lt;= height &lt;= maxHeight. CAS. // If set to true by tests, then extra delays are added to make it easier to // detect unusual race conditions. testing bool } type Arena struct { n uint64 buf []byte } 可以看到整个结构比较简单，arena 代表跳表数据存储的地方，是一个固定大小的 buffer；cmp 代表跳表内节点值的比较函数；head 为头节点；tail 为尾节点；height 代表跳表的高度（节点当前最大层数）；testing 为测试时候使用，可以忽略。
node 再来看看里面核心结构 node 的定义：
type node struct { // Immutable fields, so no need to lock to access key. keyOffset uint32 keySize uint32 valueSize uint32 allocSize uint32 tower [maxHeight]links } type links struct { nextOffset uint32 prevOffset uint32 } keyOffset 代表 key 在 arena 中的偏移，keySize 代表 key 的大小；valueSize 代表 value 的大小；allocSize 代表 node 占用的内存大小；tower 是一个数组，每个元素相当于每层的 pre-next 指针，指向各层的前一个节点和后一个节点。
可以看出 node 并未存储 key-value 的具体值，而是存的偏移和大小，value 的偏移并未存储，因为可以通过 keyOffset 和 keySize 间接计算出来。tower 比较形象，可以把节点看做塔一样，有多层，每层通过双向指针串联前后节点。因此 Skiplist 逻辑上是一个双向链表，Skiplist 在 Pebble 中的物理结构如下图：
tower 指向的 node 个数有多个，图上只画了一对 link 表示节点在某层的前后节点偏移。
方法实现 了解 Skiplist 的数据结构定义后，接下来，看看如果操作 Skiplist，即 Skiplist 的相关方法实现。
创建跳表 func NewSkiplist(arena *Arena, cmp base.Compare) *Skiplist { skl := &amp;Skiplist{} skl.Reset(arena, cmp) return skl } 创建 Skiplist 时，需要由外部传入 arena 和 cmp，Skiplist 的容量和比较函数由使用者决定。这里会调用 Reset 对 Skiplist 进行初始化操作，来看看具体实现逻辑：
func (s *Skiplist) Reset(arena *Arena, cmp base.Compare) { // Allocate head and tail nodes. head, err := newRawNode(arena, maxHeight, 0, 0) if err != nil { panic(&#34;arenaSize is not large enough to hold the head node&#34;) } head.keyOffset = 0 tail, err := newRawNode(arena, maxHeight, 0, 0) if err != nil { panic(&#34;arenaSize is not large enough to hold the tail node&#34;) } tail.keyOffset = 0 // Link all head/tail levels together. headOffset := arena.getPointerOffset(unsafe.Pointer(head)) tailOffset := arena.getPointerOffset(unsafe.Pointer(tail)) for i := 0; i &lt; maxHeight; i++ { head.tower[i].nextOffset = tailOffset tail.tower[i].prevOffset = headOffset } *s = Skiplist{ arena: arena, cmp: cmp, head: head, tail: tail, height: 1, } } 既然是初始化，那肯定是对 Skiplist 里面的成员变量进行初始化操作，arena 和 cmp 已经有了，很明显还需要对 head、tail、height 进行初始化赋值。首先通过 newRawNode 创建 head 节点和 tail 节点，由于 head 和 tail 不存储 key-value，因此 keyOffset 都初始化为 0，代表没有具体值；然后获取 head 和 tail 在 arena 中的偏移位置，并初始化 head.tower 的 nextOffset 和 tail.tower 的 prevOffset。
最后，Skiplist 中个成员变量初始化完毕，Skiplist 便算是创建成功了。接下来看看 newRawNode 的实现逻辑。
func newRawNode(arena *Arena, height uint32, keySize, valueSize uint32) (nd *node, err error) { // Compute the amount of the tower that will never be used, since the height // is less than maxHeight. unusedSize := uint32((maxHeight - int(height)) * linksSize) nodeSize := uint32(maxNodeSize) - unusedSize nodeOffset, allocSize, err := arena.alloc(nodeSize+keySize+valueSize, align4, unusedSize) if err != nil { return } nd = (*node)(arena.getPointer(nodeOffset)) nd.keyOffset = nodeOffset + nodeSize nd.keySize = keySize nd.valueSize = valueSize nd.allocSize = allocSize return } newRawNode 会在 arena 中分配 node 所需内存大小，然后将成员变量的值存入 arena 中。由于节点的高度，即 tower 的层数小于等于 maxHeight，height 到 maxHeight 之间的 links 并不会使用，这里要计算出未使用的大小，然后再用节点最大大小减去未使用大小得到节点实际大小；
知道节点的大小后，便可以在 arena 中分配相应大小的内存空间，最后对 node 成员变量初始化，初始化的值则保存在 arena 中。
newRawNode 中分配 node 的内存时，采用 4 字节对齐，原因是 node 的每个变量都是 uint32（tower 元素中的成员变量也是），因此按 4 字节对齐时，cpu 访问内存获取变量值时更加高效，提升访存性能。字节对齐的好处可参考网上其他文章。
添加 key-value func (s *Skiplist) Add(key base.InternalKey, value []byte) error { var ins Inserter return s.addInternal(key, value, &amp;ins) } func (s *Skiplist) addInternal(key base.InternalKey, value []byte, ins *Inserter) error { // 找到 key 要插入的位置 if s.findSplice(key, ins) { return ErrRecordExists } nd, height, err := s.newNode(key, value) if err != nil { return err } // node 在 arena 中的偏移 ndOffset := s.arena.getPointerOffset(unsafe.Pointer(nd)) var found bool var invalidateSplice bool for i := 0; i &lt; int(height); i++ { prev := ins.spl[i].prev next := ins.spl[i].next if prev == nil { if next != nil { panic(&#34;next is expected to be nil, since prev is nil&#34;) } prev = s.head next = s.tail } for { // 获取 prev 和 next 的偏移 prevOffset := s.arena.getPointerOffset(unsafe.Pointer(prev)) nextOffset := s.arena.getPointerOffset(unsafe.Pointer(next)) nd.tower[i].init(prevOffset, nextOffset) nextPrevOffset := next.prevOffset(i) // next 的 prevOffset 不指向 prevOffset if nextPrevOffset != prevOffset { prevNextOffset := prev.nextOffset(i) // prev 的 nextOffset 指向 next，说明 next 的 prevOffset 还没来得及修改 if prevNextOffset == nextOffset { next.casPrevOffset(i, nextPrevOffset, prevOffset) } } // cas 修改 prev 的 nextOffset if prev.casNextOffset(i, nextOffset, ndOffset) { // cas 修改 next 的 prevOffset next.casPrevOffset(i, prevOffset, ndOffset) break } // cas 失败，说明其他线程成功插入节点，需要重新计算 key 插入的位置 prev, next, found = s.findSpliceForLevel(key, i, prev) if found { if i != 0 { panic(&#34;how can another thread have inserted a node at a non-base level?&#34;) } return ErrRecordExists } invalidateSplice = true } } if invalidateSplice { ins.height = 0 } else { for i := uint32(0); i &lt; height; i++ { ins.spl[i].prev = nd } } return nil } 我们主要关注 addInternal，该方法利用循环+CAS，在并发场景下可以高效地插入节点。首先通过 findSplice 方法找到要插入的位置，位置保存在 ins 的 spl 中，该变量存储了每层的前后节点。找到位置后，创建节点并保存 key-value 值，然后再 for 循环中遍历每层的 prev 和 next，将节点插入 pre 和 next 之间，如下：
+----------------+ +------------+ +----------------+ | prev | | nd | | next | | prevNextOffset |----&gt;| | | | | |&lt;----| prevOffset | | | | | | nextOffset |----&gt;| | | | | |&lt;----| nextPrevOffset | +----------------+ +------------+ +----------------+ 插入节点时，首先将 nd 的 prevOffset 和 nextOffset 指向 prev 和 next；然后通过 for+cas 修改 prev 的 nextOffset 再修改 next 的 prevOffset，如果修改成功，退出内存循环，继续处理下一层，否则重新计算 node 在当前层的插入位置，继续重试。
注意，这里在更新 prev 的 nextOffset 和 next 的 prevOffset 前会先修正 next 的当前 prevOffset，因为可能存在另一个线程正在插入的节点是 prev，而 prev 的 nextOffset 已经指向了 next，但是 next 的 prevOffset 还未来得及指向 prev。为了保证 cas 正确性，需要帮助另一线程将 next 的 prevOffset 指向 prev。
查找 key Skiplist 的查找及遍历都是通过迭代器 Iterator 实现的，这里只列出最基础的查找方法：
func (it *Iterator) seekForBaseSplice(key []byte) (prev, next *node, found bool) { ikey := base.MakeSearchKey(key) level := int(it.list.Height() - 1) prev = it.list.head for { prev, next, found = it.list.findSpliceForLevel(ikey, level, prev) if found { if level != 0 { // next is pointing at the target node, but we need to find previous on // the bottom level. prev = it.list.getPrev(next, 0) } break } if level == 0 { break } level-- } return } 该方法逻辑非常简单，初始查找节点为 head，然后从最高层开始查找，如果找到目标节点（注：next 才是目标节点），则将 next 0 层的前一个节点赋给 prev（必须最底层才能代表真正的前节点）；如果没有找到，则继续根据 prev 节点查找下一层，直到 0 层如果还未找到，说明目标值不存在。
我们继续看下 findSpliceForLevel 的逻辑：
func (s *Skiplist) findSpliceForLevel( key base.InternalKey, level int, start *node, ) (prev, next *node, found bool) { prev = start for { // Assume prev.key &lt; key. next = s.getNext(prev, level) if next == s.tail { break } offset, size := next.keyOffset, next.keySize nextKey := s.arena.buf[offset : offset+size] n := int32(size) - 8 cmp := s.cmp(key.UserKey, nextKey[:n]) if cmp &lt; 0 { // We are done for this level, since prev.key &lt; key &lt; next.key. break } if cmp == 0 { // User-key equality. var nextTrailer uint64 if n &gt;= 0 { nextTrailer = binary.LittleEndian.Uint64(nextKey[n:]) } else { nextTrailer = uint64(base.InternalKeyKindInvalid) } if key.Trailer == nextTrailer { // Internal key equality. found = true break } if key.Trailer &gt; nextTrailer { // We are done for this level, since prev.key &lt; key &lt; next.key. break } } // Keep moving right on this level. prev = next } return } func (s *Skiplist) getNext(nd *node, h int) *node { offset := atomic.LoadUint32(&amp;nd.tower[h].nextOffset) return (*node)(s.arena.getPointer(offset)) } 从 start 节点开始（start.key &lt; key），获取 next 节点，getNext 内部通过原子操作获取 next 的偏移，有了偏移便可通过 arena 获取 buffer 中的 nextKey，然后比较 key 和 nextKey 的原始值（去掉 key 的后 8 位 Trailer，key 的格式参考之前的文章），如果 userKey 相等，接着比较 Trailer，如果相等，则找到目标节点；Trailer 越大，说明 key 越小（Trailer 大说明版本更新，应该排在前面），如果 key 的 Trailer 大于了 nextTrailer，那就每必要往后找了，因为后面的 Trailer 更小，反之，继续遍历下一节点。
到这里，暂且介绍完了 skiplist 最基本的方法 &ndash; 创建、添加、查找，根据方法的实现可以帮助大家更清晰的认识 Pebble 对 Skiplist 的实现原理及使用方式。
总结 本文主要通过对源码解析，介绍了 Pebble 如何实现一个高性能无锁的 Skiplist，并介绍了 Skiplist 的几种最基本的方法，便于加深大家对 Skiplist 的理解，也利于后续源码探索。
]]></content></entry><entry><title>Pebble 源码剖析-写入流程(续)</title><url>/2022/01/24/pebble-write-s/</url><categories><category>pebble</category></categories><tags><tag>Pebble</tag></tags><content type="html"><![CDATA[上一章，分析了 Pebble 写入的整体流程，并深入分析了 Pipeline 的三个执行阶段。但还未对 large batch 和 makeRoomForWrite 具体执行逻辑进行分析，本章将对这两点内容展开讨论。
Large Batch 这里首先思考两个问题：为什么需要区分 large batch，为什么需要对 large batch 特殊处理？笔者最初以比较疑惑，带着疑问向社区提了 issue ，大家可以看下官方开发人员的回答。
回顾下上一章 DB.Apply 方法部分代码：
if int(batch.memTableSize) &gt;= d.largeBatchThreshold { batch.flushable = newFlushableBatch(batch, d.opts.Comparer) } 这里会判断 batch 的大小，如果其超过设定的阈值，则将该 batch 当做 large batch 处理。这里会调用 newFlushableBatch 方法，根据 batch 生成一个 flushable（可以理解成 immutable memtable）。下面看一下 newFlushableBatch 具体的逻辑：
该函数代码比较冗长，这里省略部分分支逻辑，保留主要逻辑
func newFlushableBatch(batch *Batch, comparer *Comparer) *flushableBatch { b := &amp;flushableBatch{ data: batch.data, cmp: comparer.Compare, formatKey: comparer.FormatKey, offsets: make([]flushableBatchEntry, 0, batch.Count()), } if len(b.data) &gt; batchHeaderLen { // Non-empty batch. var index uint32 // 迭代 batch for iter := BatchReader(b.data[batchHeaderLen:]); len(iter) &gt; 0; index++ { offset := uintptr(unsafe.Pointer(&amp;iter[0])) - uintptr(unsafe.Pointer(&amp;b.data[0])) // 解析当前 batch record kind, key, _, ok := iter.Next() if !ok { break } entry := flushableBatchEntry{ offset: uint32(offset), index: uint32(index), } entry.keyStart = uint32(uintptr(unsafe.Pointer(&amp;key[0])) - uintptr(unsafe.Pointer(&amp;b.data[0]))) entry.keyEnd = entry.keyStart + keySize b.offsets = append(b.offsets, entry) } } pointOffsets := b.offsets sort.Sort(b) return b } 首先，初始 flushableBatch **b **，b 中包含了原始 batch 的数据；然后迭代 batch，解析 batch 的每个 record 生成 flushableBatchEntry，entry 主要包含 record 在 batch 中的偏移 offset，record 是 batch 索引号 index（用于计算当前 record 的 seqNum），record 的原始 key 在 batch 中的起始偏移和终止位置。可以看到 entry 主要记录的是 record 相关的位置和偏移信息，根据这些信息可以得到 record 中的 key 和 value。将 entry 加入到 b 的 offsets 中，迭代完毕后，b 便拥有了原始 batch 中所有 record 的位置和偏移信息。
最后对 b 进行排序，排序规则参考如下函数：
func (b *flushableBatch) Less(i, j int) bool { ei := &amp;b.offsets[i] ej := &amp;b.offsets[j] ki := b.data[ei.keyStart:ei.keyEnd] kj := b.data[ej.keyStart:ej.keyEnd] switch c := b.cmp(ki, kj); { case c &lt; 0: return true case c &gt; 0: return false default: return ei.offset &gt; ej.offset } } 可以看到 flushableBatch 的排序规则：比较 entry 间 key 的大小，比较函数默认为 bytes.Compare，如果 key 相同则偏移位置大的 key 排前面（偏移越大说明 key 是后写入，代表值越新）。
好的，到这里 flushableBatch 便准备完毕了，并会赋值给 batch 的 flushable。再回顾 DB.commitWrite，上一章把方法中 large batch 相关的逻辑省略了，如下：
repr := b.Repr() if b.flushable != nil { b.flushable.setSeqNum(b.SeqNum()) if !d.opts.DisableWAL { var err error size, err = d.mu.log.SyncRecord(repr, syncWG, syncErr) if err != nil { panic(err) } } } 接着之前的逻辑，这里 batch 的 flushable 不为空，如果启用了 wal，这些会先写 wal，而后再调用 makeRoomForWrite。这里和普通 batch 的处理顺序相反，后面会做解释。这里先看 makeRoomForWrite，该方法主要的逻辑有如下几点：
当前 memtable 空间足够，直接写入 当前 memtable 空间不够，将当前 memtable 切换为 immutable memtable，然后将当前 memtable 刷盘 batch 为空或者为 large batch 则直接切换当前 memtable（注：这两种视为非常规 batch） 如果切换 memtable 则同时会生成新的 log 文件 上述为 makeRoomForWrite 的主要逻辑，还要一些分支逻辑需要结合代码来看看（log 文件相关的代码省略）：
func (d *DB) makeRoomForWrite(b *Batch) error { force := b == nil || b.flushable != nil stalled := false for { // 检查当前 memtable 是否正在切换中 if d.mu.mem.switching { d.mu.mem.cond.Wait() continue } // batch 为正常小数据量时，进入 prepare if b != nil &amp;&amp; b.flushable == nil { err := d.mu.mem.mutable.prepare(b) if err != arenaskl.ErrArenaFull { if stalled { d.opts.EventListener.WriteStallEnd() } return err } } // force || err == ErrArenaFull, so we need to rotate the current memtable. { var size uint64 // 计算所有 memtable 大小 for i := range d.mu.mem.queue { size += d.mu.mem.queue[i].totalBytes() } // 总大小超过阈值，需要阻塞写，等待 compact 完成 if size &gt;= uint64(d.opts.MemTableStopWritesThreshold)*uint64(d.opts.MemTableSize) { if !stalled { stalled = true d.opts.EventListener.WriteStallBegin(WriteStallBeginInfo{ Reason: &#34;memtable count limit reached&#34;, }) } d.mu.compact.cond.Wait() continue } } l0ReadAmp := d.mu.versions.currentVersion().L0Sublevels.ReadAmplification() // l0 读放大超过阈值需要阻塞等待 if l0ReadAmp &gt;= d.opts.L0StopWritesThreshold { // There are too many level-0 files, so we wait. if !stalled { stalled = true d.opts.EventListener.WriteStallBegin(WriteStallBeginInfo{ Reason: &#34;L0 file count limit exceeded&#34;, }) } d.mu.compact.cond.Wait() continue } if b != nil &amp;&amp; b.flushable != nil { entry := d.newFlushableEntry(b.flushable, imm.logNum, b.SeqNum()) entry.releaseMemAccounting = d.opts.Cache.Reserve(int(b.flushable.totalBytes())) d.mu.mem.queue = append(d.mu.mem.queue, entry) imm.logNum = 0 } var logSeqNum uint64 if b != nil { logSeqNum = b.SeqNum() if b.flushable != nil { logSeqNum += uint64(b.Count()) } } else { logSeqNum = atomic.LoadUint64(&amp;d.mu.versions.atomic.logSeqNum) } var entry *flushableEntry d.mu.mem.mutable, entry = d.newMemTable(newLogNum, logSeqNum) d.mu.mem.queue = append(d.mu.mem.queue, entry) d.updateReadStateLocked(nil) if immMem.writerUnref() { d.maybeScheduleFlush() } force = false } } 可以看到，这里有两种情况会强制切换 memtable，一种是 batch 为空，代表手动 flush，另一种是 batch 为 large batch。下面循环中，首先检查当前 memtable 是否正在切换中，如果是则等待当前 memtable 切换完毕；
随后判断当前 batch 是否为常规 batch，如果为常规 batch ，则调用 memtable 的 prepare 函数判断当前 batch 空间是否足够，如果足够则直接返回，否则返回 ErrArenaFull，代表空间已满；
下面进入切换 memtable 的逻辑，到这里可以看出，有三种情况会切换 memtable：1. 手动 flush，2. Large batch 3. 当前 memtable 已满；接下来会计算当前内存所有 memtable 的空间大小，如果总大小超过停写阈值，则会阻塞写，等待 compact 完成；再接下来会判断 L0 的读放大是否超过阈值，如果超过则阻塞写，等待 compact 完成；
如果是 large batch，则会生成 flushableEntry，然后添加到 immutable queue 中；最后会将当前 memtable 切换为 immutable 并加入到 queue 中。最后将 immutable 解引用，并调用 maybeScheduleFlush 触发写入操作。
思考 这里思考几个问题：
为什么需要设计 large batch large batch 的日志写入为什么在 makeRoomForWrite 之前（和普通 batch 对比） large batch 为什么会触发 memtable 切换 这几个问题的解答可以参考笔者向官方提的 issue 以及 官方文档 对 large batch 来源说明。这里笔者参考官方的回答及自己的理解，对上述问题进行解答下。
第一个问题，根据官方描述，pebble 的 memtable 实际上是一个预分配的固定内存大小的 skiplist，因此当 batch 超过 memtable 内存大小时，无法扩容，只能将 large batch 转变为 flushable 来处理，这样间接解决 memtable 内存无法容纳 large batch 的问题。再一个原因是，如果当前 large batch 过大即使未超过 memtable 内存，将 large batch 写入 memtable，那么很快就会导致 memtable full，触发 flush，既然 large batch 很快便会触发 flush，那么不如直接将其转变为 flushable，这样也避免了 large batch 到 memtable 的拷贝开销。
第二个问题，large batch 转变为 flushable 后，在 makeRoomForWrite 中会将其当做 immutable 加入到 immutable queue 中，相当于数据写入内存中，同时可能触发 flush，因此，在数据写入内存前应该先写 log，也就是日志的写入需要在 makeRoomForWrite 前。这个逻辑和普通的 batch 处理不同，可以参考上一章。
第三个问题，既然 large batch 并未写入到当前的 memtable 中，为什么也会将当前的 memtable 切换为 immutable？这个问题要和第二个问题结合起来看，从第二个问题可以知道，large batch 和当前 memtable 其实是共用的同一个 wal，在 makeRoomForWrite 中可能会触发 large batch 的 flushable 刷盘，刷盘后其对应的 wal 理应被删除以避免日志 replay 的开销，因此 memtable 也会被牵连一起刷盘。
总结 本章主要讲解了写入流程中 pebble 对 large batch 的特殊处理方式，同时分析了 makeRoomForWrite 的处理逻辑。最后分析了为什么会产生 large batch 及针对 large batch 特殊处理的原因。
]]></content></entry><entry><title>Compaction 策略 - Leveled</title><url>/2022/01/30/compaction-leveled/</url><categories><category>lsm</category></categories><tags><tag>compaction</tag></tags><content type="html"><![CDATA[在上一章，我们介绍了 Size-Tiered Compaction 策略（STCS），讨论了 STCS 的原理，并着重分析了它的缺陷 &ndash; 空间放大。本章我们将会介绍另一种策略 &ndash; Level Compaction Strategy (LCS)，该策略能够显著降低 STCS 带来的空间放大，同时也能改善读放大带来的性能损失，但它也会带来其他负面的效果 &ndash; 写放大。本章我们将围绕空间放大、写放大、读放大，三个维度来展开分析 LCS。
Leveled Compaction Strategy 与 STCS 对比，LCS 具备如下特点：
sst 的大小可控，默认每个 sst 的大小一致（STCS 在经过多次合并后，层级越深，产生的 sst 文件就越大，最终会形成超大文件） LCS 在合并时，会保证除 Level 0（L0）之外的其他 Level 有序且无覆盖 除 L0 外，每层文件的总大小呈指数增长，假如 L1 最多 10 个，则 L2 为 100 个，L3 为 1000 个&hellip; 我们来看下 LCS 具体是如何工作的，如下图：
首先是内存中的 memtable 刷到 L0，当 L0 中的文件数达到一定阈值后，会将 L0 的所有文件及与 L1 有覆盖的文件做合并，然后生成新文件（如果文件大小超过阈值，会切成多个）到 L1，L1 中的文件时全局有序的，不会重现重叠的情况；
当 L1 的文件数量达到阈值时，会选取 L1 中的一个 sst 与 L2 中的多个文件做合并，假设 L1 有 10 个文件，那么一个文件便占 L1 数据量的 1/10，假设每层包含的 key 范围相同，那么 L1 中的一个文件理论上会覆盖 L2 层的 10 个文件，因此会选取 L1 中的一个文件与 L2 中的 10 个文件一起 compaction，将生成的新文件放到 L2；
当 L2 文件数量达到阈值时，处理方式同上，如此往复。
空间放大 介绍完 LCS 的工作原理后，我们来看下 LCS 是如何解决空间放大的。在上一章中，我们看到空间放大主要由两个原因：1. compaction 过程中，会产生临时空间占用；2. 重复数据分散在不同的 sst 中，导致空间浪费。
LCS 不会像 STCS 一样产生明显的临时磁盘占用问题。由于 STCS 存在超大 sst 的情况，因此在将这些超大 sst 合并时，会产生更大的文件，这些文件会临时同时存放在磁盘上，导致临时空间放大严重。LCS 不会有超大文件，而且在层与层之间合并时，大体上只选取 11 个 sst 进行 compaction，而这 11 个文件的大小只占整个系统的小部分，因此临时空间放大很小。
LCS 也不会像 STCS 那样，被重复数据所困扰，原因是 LCS 中除 L0 之外，其他层的数据都是有序不重叠的，因此每层内，各个 sst 间不存在重复数据。下面举例验证下 LCS 确实具备较低的空间放大。
LCS 最好的情况是，最后一层数据已经被填满。假设最后一层为 L3，一共有 1000 个 sst。那么 L1 和 L2 一共最多 110 个文件。由于每个 sst 基本大小相同，因此，几乎 90% 的数据在 L3。我们直到每层内的数据不会重复，因此最多，L1 和 L2 的数据包含在 L3 中，重复数据导致的空间放大为 1/0.9=1.11，相比上一章 STCS 8 倍的空间放大好得多。
然后，也存在比较差的情况，即当最后一层没有填满时，假设 L3 的文件个数为 100 个，L2 中的文件数也是 100 个，最坏情况下，如果 L2 和 L3 的数据相同，则会产生 2 倍的空间放大。但即使是这样，相对 STCS 还是好得多。
为了能够更直观的对比 LCS 和 STCS，Scylla 官方对 LCS 做和上一章同样的实验。我们来看看实验结果。
实验 1 实验 1 是持续写入不同的数据，如上图，可以看到，随着数的不断写入，STCS 会出现较大的毛刺，在最大一次 compaction 时，毛刺达到最巅峰，空间放大达到了 2。而 LCS 的毛刺比较小，空间放大很小，和上面的推导一致。
实验 2 实验 2 是一种极端情况，不断写入重复数据，且重复的粒度是 1.2 GB，即所有 sst 中的数据相同。如上图，可以看到 STCS 的空间放大接近 8 倍。而 LCS 的空间放大也超过了 2，原因是这里数据量不能使 LCS 产生 L3，实际只有 L0 和 L1，由于 L1 允许存在多个数据重叠的 sst，因此这里拉高了放大倍数。
读放大 读放大指的是，读取一次数据，会产生多次 io，io 次数即为读放大（注：这里是以 io 次数来做的定义，有些文章是以数据量放大倍数来定义，笔者觉得应该将两者结合起来定义更准确）。对于 STCS 来说，由于每层内的 sst 数据可以相互重叠，因此最坏情况下，需要遍历层内所有 sst 才能获取。而对于 LCS 来说，如果数据确定在某层（除 L0）的话，只需要定位到数据所在的 sst，只访问一个 sst 即可，所以，理论上来讲，LCS 的读放大要比 STCS 好得多。
写入放大 LCS 解决了空间放大问题，那么是不是就万事大吉了呢？当然不是，LCS 虽然解决了空间放大，但是也引入了另一个问题 &ndash; 写入放大，在上一章中，我们没有仔细讲解写入放大，这里给出写入放大的定义。写入放大指的是实际写入的物理数据量是写入数据量的多倍。写入放大和空间放大容易混淆，写入放大主要指的是 io 写带宽的放大，比如用户实际写入的数据是 1GB，由于某些原因导致实际的写 io 达到了 5 GB，写放大就是 5。
这里来看个例子，如上一章的实验 1，实际写入的数据是 8.8 GB，但是最后写入放大了 5 倍。我们来看看原因，由于一个 sst 设置的是 100 MB，对于 STCS 来说，L0 的文件大小是 100 MB，写满 4 （默认值）个时，compaction 到 L1，L1 的文件大小便为 400MB，以此类推，最终这 8.8 GB 在 LSM-Tree 上的分布如下：
可以看到，每份数据的流向为：内存&ndash;&gt;L0&ndash;&gt;L1&ndash;&gt;L2&ndash;&gt;L3，还有一份写入到了 WAL。因此同一份数据被写入了 5 次，很明显写入放大为 5。可以对于 STCS 来说，写入放大和层高强相关，而每层的数据量又是呈指数增长的（即第一层最多 400MB，第二层 1600MB，第三层6400MB，一次类推），很明显层高和数据量的关系为对数关系，而写入放大和层高一致，因此写入放大随数据量增长为 O(logN)，N 为数据量大小。
但是对于 LCS 来说，情况会更糟糕，我们来分析下原因：假设需要将 L1 层的数据量 X compaction 到下一层，STCS 的写如放大为 1。而 LCS 需要将这 X 的数据量和 L1 层 10倍 X 数据量，一共 11 * X 一起 compaction，写入放大为 11，是 STCS 的 11 倍。
当然情况也不是都这么极端，在数据频繁修改，写数据写入频率较低的场景下，数据往往到不了更深的 Level，大部分 compaction 是 L0 &ndash; &gt; L1，由于 L0 和 L1 的数据量没有严格的倍数关系，因此往往 L0 到 L1 的写入放大相对较小。
总体来说，由于 LCS 为了保证 L0 以下的 Level 内数据不重复，所以在 compaction 时，对导致更高的写入放大。而由于写放大抢占磁盘带宽进而又会影响读性能，因此，在写入（非更新、删除）频繁的场景下，LCS 可能不是最佳的选择，甚至不如 STCS。
总结 本文介绍了 Leveled Compaction 的工作原理，并从空间放大、读放大、写入放大三个维度分别对比了 LCS 和 STCS 的优缺点。下章将继续介绍另外的 compaction 策略。
References Scylla’s Compaction Strategies Series: Write Amplification in Leveled Compaction https://en.wikipedia.org/wiki/Write_amplification ]]></content></entry><entry><title>Compaction 策略 - Size-Tiered</title><url>/2022/01/27/compaction-size-tired/</url><categories><category>lsm</category></categories><tags><tag>compaction</tag></tags><content type="html"><![CDATA[开篇 本系列文章主要介绍 LSM-Tree 中非常重要的技术点 &ndash; Compaction。理解 Compaction 的作用及工作机制，对基于 LSM-Tree 类型的数据库开发或者调优有极大的益处。本系列文章将结合业界大佬的博文和笔者自己的思考总结对 compaction 运作机制进行详细讨论，本文为第一篇，主要介绍 Compaction 的定义、由来、工作原理等，同时也会介绍 compaction 的其中一种策略 &ndash; Size-Tiered。
Compaction compaction 是什么？为什么需要 compaction？在回答这两个问题前，先简单介绍下 LSM-Tree (Log Structured Merge Tree)。LSM-Tree 设计的初衷是为了达到高效地写入及良好读取数据的目的，基于 LSM-Tree 的数据库写入更新都是先将数据写入到内存中的 memtable，当其达到一定阈值后，转变为 immutable memtable，然后刷到磁盘文件 sst（sorted string table）中，sst 是不可修改的，数据的更新和删除都是以写入新记录的形式呈现。数据在文件中是按 key 有序组织的，利于高效地查询和后续合并。
随着数据的不断写入和更新，sst 的数量会不断增加，进而会出现两个问题：1. sst 中可能存在修改前的老数据和已经删除的数据，这些无用数据会占用存储空间，造成资源浪费；2. 由于 sst 越来越多，数据分散在多个文件，读取时，可能会访问多个文件，导致读性能下降。对于上述问题，需要一些机制去解决，这种机制就称为 compaction，compaction 的目的是将多个 sst 合并成一个，在合并的同时将无用的数据清理掉，合并成的新文件也是按 key 排序的。可以看到，通过 compaction，上述很好的解决上述问题。
现在，我们知道什么是 compaction 及 compaction 的作用了。但是 compaction 应该以什么策略来选择需要合并的 sst，以及策略产生效果如何？进入本文的主题 Size-Tiered Compaction。
Size-Tiered Compaction 从名称可以看出，这种策略和大小有关，没错，Size-Tiered Compaction Strategy (STCS) 的思路就是将大小相近的 sst merge 成一个新文件。如下图：
memtable 逐步刷入到磁盘 sst，刚开始 sst 都是小文件，随着小文件越来越多，当数据量达到一定阈值时，STCS 策略会将这些小文件 compaction 成一个中等大小的新文件。同样的道理，当中等文件数量达到一定阈值，这些文件将被 compaction 成大文件，这种方式不断递归，会持续生成越来越大的文件。
总的来说，STCS 就是将 sst 按大小分类，相似大小的 sst 分在同一类，然后将多个同类的 sst 合并到下一个类别。通过这种方式，可以有效减少 sst 的数量。由于 STCS 策略比较简单，同一份在数据 compaction 期间拷贝的次数相对较少，即写入放大相对小（和其他策略的 compaction 对比，在下章介绍），很多基于 LSM-Tree 的系统将其作为默认的 compaction 策略，如 Lucene、Cassandra、Scylla 等。
STCS 逻辑简单、写入放大低，但是它也有很大的缺陷 &ndash; 空间放大。其实也存在较大的读放大，这个放在下章介绍。
空间放大（Space Amplification） 空间放大指的是 compaction 的过程中，会导致数据膨胀，需要比原始数据更大的存储空间。对于机械硬盘来说，由于价格便宜，空间放大产生的代价相对可控。当时现在最主流的磁盘是 SSD（固态硬盘），SSD 的性能比机械硬盘好很多，但是价格也贵得多，因此，对于跑在 SSD 上的系统，空间放大无疑会带来很大的成本开销。
为了证明 STCS 带来的空间放大问题，Scylla 官方做了两个实验，我们一起来看看。
实验 1 该实验采用只写的方式向 Scylla 单节点持续写入一共 9GB（磁盘表现） 的数据，实验理想情况下应该看到磁盘空间使用按时间呈直线上涨的趋势图，但是结果如下：
从上图可以看到，由于 compaction 带来的空间放大，会出现很多毛刺。刚开始毛刺比较小，是因为刚开始都是小文件，慢慢产生很多中等文件，再产生大文件，最后会产生超大文件，在 2000 时间点时，这些超大文件触发完全 compaction，磁盘使用几乎是当前数据的两倍。
那么为什么会产生空间放大呢？compaction 的过程中，参与 compaction 的 sst 不能立马删除，直到新生成的 sst 写入完毕，这里其实还有一个原因，如果老的 sst 有读操作，由于文件还被引用，也是不能立即删除的。因此，在 compaction 的过程中，磁盘上新老文件共存，产生临时空间放大。即使这种空间放大是临时的，但是对于系统来说，不得不使用比实际数据量更大的磁盘空间，以保证 compaction 正常执行，这产生的代价很昂贵。
本实验假设一直写入的数据不重复，空间放大最坏情况下为 2。这种情况还能接受，但是实际上，还会有更糟糕的情况，我们来看下 Scylla 官方做的第二个实验。
实验 2 这个实验的做法是，刚开始写入一定的数据量触发 flush，产生第一个 sst，随后，继续按照这个数据量重复写数据。当磁盘文件个数达到 4 （Scylla 默认触发条件）个时，触发 compaction，compaction 的过程中产生的最大空间放大为 5，因为会存在 5 份数据相同的文件（4个老文件，1个新文件）。但是随着写入的继续，实际情况可能更糟糕，因为同一份数据可能不仅在当前最大级别的 4 个文件中，还可能存在在更个更低级别的文件中，每个文件都是同一份数据的 copy，因此实际上存在多少文件就有多个的空间放大。我们来看下实现的结果：
这是持续写入了 15 次相同数据的结果，可以看到最终的实际数据量是 1.2 GB，但是在中途 compaction 的过程中，最大的磁盘空间占用达到了 9.3 GB，空间放大接近惊人的 8 倍。因此，在数据覆盖写的场景下，空间放大极其严重。
通过 Scylla 官方的这两个实验，对于覆盖写较少的场景，STCT 的空间放大尚可接受；但是对于覆盖写频繁的场景，STCT 便不再是一个很好的选择。因此，针对空间放大问题，业界大佬们有提出了新的解决方案，我们下章介绍。
总结 本文主要介绍了 compaction 的定义及作用，并介绍了最简单的一种 compaction 策略 &ndash; Size-Tiered。分析了 STCT 的优缺点，并结合 Scylla 官方的实验结果，直观展示了 STCT 空间放大的缺陷。通过对 compaction 策略的分析，对于我们开发或者理解基于 LSM-Tree 的数据库系统有很大帮助，下章将继续介绍另一种 compaction 方案。
Reference： Scylla’s Compaction Strategies Series: Space Amplification in Size-Tiered Compaction ]]></content></entry><entry><title>Pebble 源码剖析-写入流程</title><url>/2022/01/24/pebble-write/</url><categories><category>pebble</category></categories><tags><tag>Pebble</tag></tags><content type="html"><![CDATA[本章会结合 Pebble 源码来剖析整个写入流程，并会深入分析整个写入路径上涉及的一些技术细节。这里说明，文章只会贴一些关键代码和代码行数较少的函数或者方法，尽量避免太多代码内容影响阅读感受。
预备知识 为了便于大家理解，首先会科普一些基础概念，有基础的同学可以直接跳过。为了更直观的理解，我们先将 上一章 的图搬过来。
WAL Wal（Write ahead of log）即预写日志，其是一种日志结构，每条日志包含了数据的原始内容并以追加写的方式写入磁盘文件，在数据写入内存前需将原始数据写入到 Wal。Wal 的写入模式可以分为 Sync（同步）和 Async（异步），如果为同步写则前台需阻塞等待日志落盘，异步写只需将日志记录写到内存缓存后便可返回。因此，同步模式可靠性更高但性能偏低，异步模式性能很好但有丢数据的风险，需根据具体场景合理选择使用哪种模式。Wal 的具体格式会在下面源码分析阶段详细介绍。
Memtable Memtable 是数据（KV Pairs）在内存中的载体，业务数据写入 Pebble 后，首先写 Wal，然后会写入到 Memtable，数据在 Memtable 中会按序存储。Memtable 有两种形式，一种是 Mutable Memtable（可变），另外一种是 Immutable Memtable（不可变），顾名思义，前者可以同时支持读写，而后者只能读不能写。Memtable（默认指是 Mutable）一般会有大小限制，写满后会转变为 Immutable，Immutable 会由专门后台线程按照某种策略刷到磁盘，同时释放内存并清理对应的 Wal。
SSTable SSTable 即 Sorted String Table 简称为 SST，由内存中的 Memtable 刷盘而成，是一种有序的、不修改的磁盘文件格式，其中 Key 和 Value 都可以是任意的 byte 序列。和 RocksDB/LevelDB 一样，SST 在 Pebble 中也是按层组织，数据首先刷入到第 0 层，在到达一定数量后后被后台线程 Merge 并 Compact 到下层。每层的数据量按倍数增长，越往下层数据量越大，同时数据也越老。
由于本章只分析写入流程，因此我们先科普上述三个比较重要的概念，其他知识后面涉及再详细展开。
源码分析 有了上面的预备知识，本节可以开始对源码进行剖析了。首先贴上官方的 Demo 程序：
package main import ( &#34;fmt&#34; &#34;log&#34; &#34;github.com/cockroachdb/pebble&#34; ) func main() { db, err := pebble.Open(&#34;demo&#34;, &amp;pebble.Options{}) if err != nil { log.Fatal(err) } key := []byte(&#34;hello&#34;) if err := db.Set(key, []byte(&#34;world&#34;), pebble.Sync); err != nil { log.Fatal(err) } value, closer, err := db.Get(key) if err != nil { log.Fatal(err) } fmt.Printf(&#34;%s %s\n&#34;, key, value) if err := closer.Close(); err != nil { log.Fatal(err) } if err := db.Close(); err != nil { log.Fatal(err) } } 可以看到 Pebble 的使用还是比较简单，首先根据配置项打开一个 DB，然后便可以向该 DB 进行读写操作了。这里顺便介绍配置项 Options 的一些常用且重要的选项，便于后续理解。
BytesPerSync // 控制 sstable 按照该值平滑刷盘，防止系统一次性刷大量脏页导致写入延时抖动，默认值是 512KB WALBytesPerSync // 控制 wal 按照该值平滑刷盘，不过 Pebble 认为该值一般情况下没必要设置，因为大部分场景 wal 是 sync 写入的，默认值是 0 DisableWAL // 是否关闭 Wal，该参数为 true 时，不写 Wal，数据可靠性最低，默认为 false L0CompactionThreshold // level 0 读放大 compaction 阈值，到达阈值后将触发 L0 compaction，默认值是 4 L0StopWritesThreshold // level 0 的读放大停写阈值，达到阈值后将阻塞写，默认值是 12 LBaseMaxBytes // level 0 compaction 至的 level 最大容量，其他层的最大容量会根据该值动态计算，默认是 64 MB MemTableSize // memtable 的最大值，memtable 的大小从 256KB 开始分配，每次新建则翻倍直到达到该阈值，默认是 4MB MemTableStopWritesThreshold // 所有 memtable 总大小达到 MemTableStopWritesThreshold*MemTableSize 会阻塞写，默认值是 2 MaxConcurrentCompactions // 最大 compaction 并发数，默认值是 1，具体用处等到后面将 compaction 时再详细展开 ReadOnly // DB 以只读方式打开，后台的 compaction 和 flush 会关闭 了解上述基本配置项后，现在可以正式进入写入流程。这里展示不分析 Open 流程，等介绍完前几章内容再倒过来分析会更容易理解。我们直接从 DB.Set 开始。
DB.Set 数据的写入方式有两种，一种是单 KV 写入，另一种是 Batch 批量写入。单 KV 写入也会转为 Batch 方式，因此我们以 DB.Set 方法为入口来看看里面的实现逻辑。
func (d *DB) Set(key, value []byte, opts *WriteOptions) error { b := newBatch(d) _ = b.Set(key, value, opts) if err := d.Apply(b, opts); err != nil { return err } // Only release the batch on success. b.release() return nil } 可以看到，Set 方法内部会新建一个 Batch，并将 KV 塞入到 Batch，然后执行 DB 的 Apply 方法，如果成功执行则调用 release 释放 Batch。这里讲到 Batch，那我们来看看 Batch 的格式到底长啥样。
|- header -|- body -| +-----------+-----------+----...--+ |SeqNum (8B)| Count (4B)| Entries | +-----------+-----------+---------+ Batch 内部数据由 header 和 body 组成，header 包含 8 字节 SeqNum 和 4 字节 Count，SeqNum 表示 batch 的序列号，Count 表示 Entry 个数。body 由多个 Entry 构成。每个 Entry 格式如下：
+----------+--------+-----+----------+-------+ |Kind (1B) | KeyLen | Key | ValueLen | Value | +----------+--------+-----+----------+-------+ Kind 表示 Entry 的类型，如 SET、DELETE、MERGE 等，这里写入的类型为 SET。KeyLen 表示 Key 的大小，为 VInt 类型，最大 4 字节，Key 即为 KeyLen 字节序列，同理，ValueLen 表示 Value 大小，为 VInt 类型，最大 4 字节，Value 为 ValueLen 字节序列。
了解完 Batch 格式后，我们继续往下走，这里直接进入到 DB 的 Apply 方法。
DB.Apply Apply 方法里面会做一些检验工作，随后将 Batch 提交写入，这里贴部分关键代码：
if int(batch.memTableSize) &gt;= d.largeBatchThreshold { batch.flushable = newFlushableBatch(batch, d.opts.Comparer) } if err := d.commit.Commit(batch, sync); err != nil { // There isn&#39;t much we can do on an error here. The commit pipeline will be // horked at this point. d.opts.Logger.Fatalf(&#34;%v&#34;, err) } 首先会判断 batch 的大小，如果超过最大 large batch 的阈值，则该 batch 被视为 large batch，需要特殊处理。为了避免本章内容过于繁杂，本章后面所有涉及 large batch 的逻辑都将省略，下章会单独对 large batch 进行讲解。下面我们直接进入 Commit 方法。
commitPipeline.Commit 笔者认为 Commit 方法是写入流程中非常核心的方法，里面体现了 Pebble 高性能设计之道。我们先来看看代码（该方法不算特别长，全部奉上）：
func (p *commitPipeline) Commit(b *Batch, syncWAL bool) error { if b.Empty() { return nil} // 并发控制，sem 是缓冲 channel，因此可以并发 commit p.sem &lt;- struct{}{} // prepare 主要做的事情：1. 准备好可用的 memtable 2. 写 wal(可以是异步的，将 wal 塞入 queue, 再异步写，提高并发性能) // prepare 中会对 pipeline 加锁，因此整个过程是串行执行，不过该函数通常很快 mem, err := p.prepare(b, syncWAL) if err != nil { b.db = nil // prevent batch reuse on error return err } // 将 batch 写入 memtable，这里可以是并发执行，该流程是 pipeline 中最耗的 if err := p.env.apply(b, mem); err != nil { b.db = nil // prevent batch reuse on error return err } // Publish the batch sequence number. p.publish(b) &lt;-p.sem if b.commitErr != nil { b.db = nil // prevent batch reuse on error } return b.commitErr } Commit 方法包含三个核心步骤，首先是通过 prepare 方法准备好可用的 Memtable，并将数据异步写入 Wal，整个 prepare 方法会加锁，因此该方法只能串行执行，但是该方法执行较快，时间复杂度为 O(1) ；然后调用 apply 方法将 batch 写入 Memtable，由于 Memtable 采用无锁 Skiplist 实现，可以并发执行，但是该流程相对更耗，时间复杂度为 nO(logm)（n 为 Batch 记录条数，m 为 Memtable 中 key 数目）；最后调用 publish 方法将 batch 的 SeqNum 发布出去使其可见，换句话说，就是让提交的数据可读，该函数可并发执行，如果 Wal 为异步落盘，该方法会比较快。在多线程场景下，这三个阶段会被组织成 Pipleline 方式处理，我们先根据下图来直观感受下 Pipeline 的执行过程：
如上图，假设有多个线程并发执行 Commit，只有 prepare 阶段间多个线程是串行执行，其他阶段是可以并发执行的，这种思想和处理器指令流水线如出一辙。这种模型可以充分发挥现代 CPU 多核的优势。我们可以通过一个公式来计算下每个线程的平均耗时。假设每个线程的 prepare 阶段耗时为 x，apply 耗时为 y，publish 耗时为 z。则从第 1 个线程到第 n 个线程执行完毕，时间轴上总耗时为 n * x+y+z，每个线程的平均耗时为 (n * x+y+z)/n = x+(y+z)/n。可以看出在理想情况下，并发量无穷大时，线程的平均执行时间趋近于 x，当然实际上线程数过多并不一定更优，因为线程切换也是有开销的，总的来说，在合理范围内，并发量越大系统吞吐也更大。
到这里，Pipeline 的设计思想就分析完成了。接下来我们再展开讲讲每个阶段的执行逻辑。
commitPipeline.prepare prepare 方法主要是准备 batch 写入的 Memtable 及异步写 wal。主要代码如下：
count := 1 if syncWAL { count++ } // commit 为 sync.Group，用于等待 batch publish，如果 wal 为同步模式，也会等待 wal 刷盘 b.commit.Add(count) p.mu.Lock() // 将 batch 如队列，保证并发场景下 batch 的顺序 p.pending.enqueue(b) // 设置 batch 的序列号，batch 的 n 条记录序列号递增 b.setSeqNum(atomic.AddUint64(p.env.logSeqNum, n) - n) // 将数据写入 wal mem, err := p.env.write(b, syncWG, syncErr) p.mu.Unlock() 首先会根据 Wal 是否为同步模式来决定 commit 的等待计数，初始计数为 1，是因为必须等待 batch 的发布（后面 publish 方法中会看到），如果 Wal 为同步模式，还必须等待 Wal 刷盘完成。注意，这里只是计数，commit.Wait 会在 publish 中调用。
然后加锁进入临界区，在临界区内，先将 batch 入队列；然后给 batch 分配递增的序列号，由于外面有上锁，因此在并发环境下，batch 在队列中的顺序和 SeqNum 的顺序关系一致，即先入队列的 SeqNum 越小；最后将数据写入 wal。这里的 write 方法其实是 DB.commitWrite，我们来看下写入的逻辑。
DB.commitWrite 这个方法里面会执行两个核心的操作：1. 准备 batch 的 Memtable；2. 将数据写入到日志的内存结构中。贴下关键代码：
// 获取 batch 的底层字节数组数据 repr := b.Repr() // 上锁，操作 memtable d.mu.Lock() err := d.makeRoomForWrite(b) mem := d.mu.mem.mutable d.mu.Unlock() // 如果 wal 未开启，直接返回 memtable if d.opts.DisableWAL { return mem, nil } // 将数据写入 wal 内存结构 if b.flushable == nil { size, err = d.mu.log.SyncRecord(repr, syncWG, syncErr) if err != nil { panic(err) } } makeRoomForWrite 主要是为了确保当前 Memtable 是否足以容纳 batch 的数据，如果当前 Memtable 容量已经满了，会将其转变为 Immutable 并重新创建 Memtable。由于 makeRoomForWrite 会对 memtable 和 log 进行操作，因此这里会加锁，该方法执行逻辑比较复杂，这里不详细展开，我们放到下章和 large batch 一起讲解。
如果开启了 Wal，会将数据写入日志内存结构。这里我们来看看日志的格式：
|- header -|- body -| +---------+-----------+-----------+----------------+--- ... ---+ |CRC (4B) | Size (2B) | Type (1B) | Log number (4B)| Payload | +---------+-----------+-----------+----------------+--- ... ---+ 日志由 header 和 body 构成。header 包含 4 字节 CRC 校验码，2 字节 Size 表示 body 的大小，1 字节 Type 表示日志处在 block 中的位置，后面详细解释， 4 字节 LogNum 表示日志文件的编号，可用于日志复用，这个后面章节再详解；payload 表示日志的内容，在这里即为 Batch 的字节数组数据。
日志是按照 32KB 的 Block 来存放的，如下图所示：
如果一条日志比较小，足以放入到 1 个 Block 中，此时 Type 即为 full，如果一条日志比较大，那么 1 个 Block 无法放入，那么一条日志便会切分成多个片段跨多个 Block 存放，第一个片段的 Type 为 first，中间片段的 Type 为 middle，最后一个片段的 Type 为 last。读取日志时，便可根据 Type 将日志组装还原。
有了上面对日志格式的讲解，我们再看 SyncRecord 方法就比较容易了，LogWriter.SyncRecord 的代码如下：
func (w *LogWriter) SyncRecord(p []byte, wg *sync.WaitGroup, err *error) (int64, error) { // 切分数据放到片段中 for i := 0; i == 0 || len(p) &gt; 0; i++ { p = w.emitFragment(i, p) } // wg 不为空，则表示 wal 是同步落盘，因此需要通知 flusher 去刷盘 if wg != nil { f := &amp;w.flusher f.syncQ.push(wg, err) f.ready.Signal() } offset := w.blockNum*blockSize + int64(w.block.written) return offset, nil } SyncRecord 方法还是比较好理解，for 循环中便会将数据按照上面讲的方式切分片段，并将片段写入 block 中。如果日志为同步落盘方式，还会通知 flusher 去刷盘，同时会将 wg 放到 sync 队列，flusher 会通过 wg 异步通知 Pipeline 刷盘完成。
到这里，通过 prepare 阶段已经将 Memtable 准备完毕，同时讲数据写入到 Wal 内存 Block 中，Pipeline 流程便会拿着准备好的 Memtable 进行写入操作，我们继续看下一个阶段 apply。
commitPipeline.commitEnv.apply 上一阶段已经准备好 Memtable，在 apply 阶段便会将 batch 写入到 Memtable 中。apply 的代码如下：
func (d *DB) commitApply(b *Batch, mem *memTable) error { // 写入 memtable，无锁 err := mem.apply(b, b.SeqNum()) if err != nil { return err } // if mem.writerUnref() { d.mu.Lock() d.maybeScheduleFlush() d.mu.Unlock() } return nil } 这里省略了部分代码，commitAppy 方法中，首先会将 batch 写入 memtable，memtable 内部是无锁 Skiplist，支持并发读写；写入完毕后释放 memtable 的写引用，最后调用 DB.maybeScheduleFlush 决定是否将 memtable flush 到磁盘，该方法是异步执行的，因此临界区耗时较短，flush 的流程我们放到后面剖析 compaction 的章节去讲。
apply 完毕后，数据就已经成功写入到 memtable 中，这时候写入流程还并未结束，数据还不能读取到。我们继续看下一阶段 publish。
commitPipeline.publish 回顾下 prepare 阶段，batch 的 commit 被计数，commit 为 sync.Group，主要用途是：1. Wal 如果是同步落盘，需等待 wal 落盘完毕；2. 等待 batch 的 SeqNum 被发布。接下来看看 publish 的实现逻辑：
func (p *commitPipeline) publish(b *Batch) { // 标记当前 batch 已经 apply atomic.StoreUint32(&amp;b.applied, 1) for { // 从队列取出 batch，该 batch 可能是当前 batch，也可能是其他线程提交的 batch t := p.pending.dequeue() if t == nil { // 关键，1. 等待 SeqNum 被发布 2. 如果 wal 同步落盘等待 flusher 通知落盘 b.commit.Wait() break } if atomic.LoadUint32(&amp;t.applied) != 1 { panic(&#34;not reached&#34;) } // 通过循环 + cas 的方式更新当前可见的 SeqNum for { curSeqNum := atomic.LoadUint64(p.env.visibleSeqNum) newSeqNum := t.SeqNum() + uint64(t.Count()) if newSeqNum &lt;= curSeqNum { // t&#39;s sequence number has already been published. break } if atomic.CompareAndSwapUint64(p.env.visibleSeqNum, curSeqNum, newSeqNum) { // We successfully published t&#39;s sequence number. break } } t.commit.Done() } } 执行到 publish 方法，说明当前 batch b 已经被 appy 到 Memtable 中了，这里先将其标记为 applied 状态，然后从队列取出队头 batch，注意：1. 取出的 batch 有可能是当前线程对应的 batch，也有可能是其他线程的 batch；2. 如果队头 batch 并未 apply，则其并不会出队列，同时返回 nil 。如果返回的是 nil，则调用 commit.Wait 等待 SeqNum 发布和 Wal 落盘，否则，通过循环 + CAS 的方式更新整个 DB 的 visibleSeqNum。
我们看下第二层 for 循环中更新 visibleSeqNum 的逻辑，首先通过原子操作取出 visibleSeqNum，然后根据 t 计算新的 SeqNum，如果新的 SeqNum &lt; visibleSeqNum，说明有排在 t 后面的 batch 已经被其他线程 publish 了，那么 t 也就相当于 publish 了，直接退出循环，否则更新 visibleSeqNum。t 成功 publish 后调用 t.commit 将计数减一，而后回到第一层循环继续消费 pengding 队列。
可以看到，publish 设计得比较有意思，支持多个消费者同时消费 pending 队列，而且每个线程可以消费其他线程的 batch。这样做的好处是每个线程不必等着处理自己的 batch，多个线程可以接连不断地消费 pending 队列并独立 publish batch，充分利用多核优势提升性能。
到此，pipeline 的三个阶段便分析完成，整个写入流程也就结束了。下一章将继续补充写入路径上对 large batch 的特殊处理，以及详细讲解 makeRoomForWrite 的执行逻辑。
总结 本章从源码角度出发，按照源码执行流程，对写入流程进行了剖析并着重讲解了 Pipeline 的三个阶段。Pebble 利用 Pipeline、异步处理、CAS 无锁编程、多线程等多种技术手段打造出了高效的写入性能。本章并未覆盖到所有细节点，欢迎感兴趣的同学多多交流。
]]></content></entry><entry><title>初识Pebble</title><url>/2022/01/20/pebble-intr/</url><categories><category>pebble</category></categories><tags><tag>Pebble</tag></tags><content type="html">Pebble 来源 Pebble 是 Cockroach 参考 RocksDB 并用 Go 语言开发的高性能 KV 存储引擎。一直以来 CockroachDB 以 RocksDB 作为底层存储引擎，虽然 RocksDB 是一款非常优秀的 KV 内嵌式数据库，但是在 Cockroach 的使用过程中，也遇到了一些使用上的缺陷及性能问题（参考： Cockroach 官方文档 ）。由于存储引擎是数据库中非常核心的组件，为了更好的把控核心技术，Cockroach 打算自研底层存储引擎，于是乎，Pebble 呼之欲出。
Pebble 功能点 Pebble 在功能及存储结构上参考了 RocksDB，但是并未完全实现 RocksDB 的功能点，从功能上来看 Pebble 相当于 RocksDB 的子集。Pebble 具备的功能点如下（来源于 官方wiki ）：
Block-based tables Checkpoints Indexed batches Iterator options (lower/upper bound, table filter) Level-based compaction Manual compaction Merge operator Prefix bloom filters Prefix iteration Range deletion tombstones Reverse iteration SSTable ingestion Single delete Snapshots Table-level bloom filters RocksDB 具备但是 Pebble 不具备的功能点：
Backups Column families Delete files in range FIFO compaction style Forward iterator / tailing iterator Hash table format Memtable bloom filter Persistent cache Pin iterator key / value Plain table format SSTable ingest-behind Sub-compactions Transactions Universal compaction style Pebble 存储结构 Pebble 和 RocksDB 一样底层的数据存储结构使用的是经典的分层结构的 LSM-Tree。熟悉数据库的同学应该比较清楚，在 NoSql 领域，LSM 被广泛使用，如 HBase、LevelDB、InfluxDB、Elasticsearch 等都使用类似的结构，可见，该结构在工程实践上还是非常靠谱的。下面简单介绍下 LSM。
如上图，LSM 的结构主要由内存中的 Memtable（含 Immutable）和磁盘中的 sstable 构成，另外，磁盘上还有其他三种比较重要的文件：Wal（预写日志）、Mainifest（sst 的元数据，记录 sst 集合的状态变化）、Current（指向当前版本的 Mainifest）。数据写入时，首先写到 Wal，保证数据的可靠性；然后再写 Memtable，待 Memtable 写满后转化为只读的 Immutable Memtable，等满足一定条件后刷到磁盘形成 sst 文件。这是 LSM 构成的大体流程，这里不具体展开，详细的读写流程后面会根据源码深入剖析。
性能对比 Pebble 是受 RocksDB 启发的 KV 存储引擎，作为后起之秀，具备极高的性能。官方利用 YCSB （由雅虎开发的通用数据库测试工具）对Pebble 和 RocksDB 进行了测试对比，测试了六种工作负载：工作负载 A 是 50％ reads 和 50％ updates 混合；工作负载 B 是 95％ reads 和 5％ updates 混合；工作负载 C 是 100％ reads；工作负载 D 是 95％ 的reads 和 5％ 的 inserts。工作负载 E 是 95％ scans 和 5％ inserts；工作负载 F 是 50％ 的 reads 和 50％ 的 read-modify-writes.。Pebble 和 RocksDB 的配置选项类似。测试结果如下（来源 官方文档 ）：
从上面的结果可以看出，pebble 的表现非常出色，即使和性能极致的 RocksDB 对比也不逊色。笔者目前尚未对最新版的 Pebble 和 RocksDB 做过测试，后面会将最新版测试结果附上。
总结 本章主要介绍了 Pebble 诞生的来源，并介绍了 Pebble 具备的功能点，然后大致讲解了 LSM 的整体结构，并未详细解决其中的技术细节，后面会根据 Pebble 源码针对每块技术点详细展开。最后给大家附上了官方的测试结果，整体看来 Pebble 表现不俗，值得我们继续深入学习和探索。</content></entry><entry><title/><url>/elasticsearch-%E8%81%9A%E5%90%88%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90/</url><categories/><tags/><content type="html"> Elasticsearch是一个高性能、可伸缩的分布式全文搜索和分析引擎，支持聚合查询功能。聚合查询是一种将搜索结果进行分组并计算聚合度量的方法，可以实现统计分析、数据挖掘等复杂查询需求。本文将剖析Elasticsearch聚合查询的原理。
聚合查询的基本概念 在 Elasticsearch 中，聚合查询是由聚合器（Aggregator）构成的。聚合器是一种计算文档集合中某些属性的统计信息的组件。它们可以嵌套在一起形成复杂的聚合查询。聚合查询的结果可以作为一个或多个值返回给用户，也可以作为一个或多个分桶（Bucket）返回给用户。分桶是一种将文档按照某些属性进行分组的方法。每个分桶包含一个文档集合，该文档集合符合某些过滤条件。
聚合查询类型 在 Elasticsearch 中，聚合查询可以分为以下几类：
指标聚合器 指标聚合器（Metric Aggregator）用来计算文档属性的统计信息，如最小值、最大值、平均值、总和、数量等等。指标聚合器包括：
Avg Aggregation：计算文档属性的平均值。 Cardinality Aggregation：计算文档属性的基数（即不同值的数量）。 Max Aggregation：计算文档属性的最大值。 Min Aggregation：计算文档属性的最小值。 Sum Aggregation：计算文档属性的总和。 Value Count Aggregation：计算文档属性的值数量。 桶聚合器 桶聚合器（Bucket Aggregator）用来将文档集合分成多个桶，每个桶都符合某些过滤条件。桶聚合器包括：
Date Histogram Aggregation：按时间段将文档分成多个桶。 Date Range Aggregation：按时间范围将文档分成多个桶。 Geo Distance Aggregation：按地理位置将文档分成多个桶。 Geo Hash Grid Aggregation：按地理位置的格子将文档分成多个桶。 Histogram Aggregation：按数字范围将文档分成多个桶。 Range Aggregation：按数字范围将文档分成多个桶。 Terms Aggregation：按文档属性将文档分成多个桶。 管道聚合器 管道聚合器（Pipeline Aggregator）用来将其他聚合器的结果作为输入，计算新的聚合结果。管道聚合器包括：
Average Bucket Aggregation：计算每个桶中文档属性的平均值。 Bucket Script Aggregation：使用自定义脚本计算桶的聚合结果。 Bucket Selector Aggregation：根据自定义条件过滤桶。 Derivative Aggregation：计算桶聚合器结果的一阶导数。 Max Bucket Aggregation：找到某个桶聚合器结果最大的桶。 Min Bucket Aggregation：找到某个桶聚合器结果最小的桶。 Sum Bucket Aggregation：计算某个桶聚合器结果的总和。</content></entry><entry><title/><url>/io-%E7%B3%BB%E5%88%97standard-io/</url><categories/><tags/><content type="html">该系列主要对文件系统 io 进行分析总结，分为如下几章：
文件系统io基础 mmap 异步io: AIO, io_uring Vectored io mmap 是银弹吗？：论文解读 文件系统 io 是数据密集型系统的核心操作，对于数据库内核开发人员来说，理解 io 工作原理是特别重要的。本章主要介绍文件系统 io 基础知识，并介绍使用最频繁的 io 类型。
xxx 块设备（Block Device）是一种按照固定大小的块进行读写的设备，通常是以扇区（Sector）为单位，每个扇区大小为 512 字节或更多。块设备允许操作系统随机地读取或写入块中的数据，因为数据存储在固定大小的块中，而且块设备一般是能够缓存数据的，这使得块设备的访问速度比字符设备更快。
块设备通常包括机械硬盘、固态硬盘、光盘、USB存储设备等，它们通过操作系统的块设备驱动程序来与计算机进行通信。在操作系统中，文件系统一般使用块设备来存储和读取数据，因为块设备支持随机访问和缓存，使得文件系统可以快速地读取和写入数据。
扇区（Sector）/块（Block）/页（Page） 扇区是块设备传输的最小单元，大多数硬盘的扇区大小为 512 字节。块是由多个相邻扇区组合而成，是设备驱动程序操作的单位，典型的块大小为 512, 1024, 2048 和 4096 字节。</content></entry><entry><title>站点示例</title><url>/flinks.html</url><categories/><tags/><content type="html">如想交换本站友情链接，请在评论区留下你的站点信息，格式参考如下：
- name: Hugo-NexT desc: Hugo NexT 官方预览网站。 avatar: https://hugo-next.eu.org/imgs/hugo_next_avatar.png link: https://hugo-next.eu.org</content></entry></search>